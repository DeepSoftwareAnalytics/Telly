10/09/2022 07:01:40 - INFO - __main__ -   device: cuda, n_gpu: 1
10/09/2022 07:01:41 - DEBUG - filelock -   Attempting to acquire lock 140389248221344 on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
10/09/2022 07:01:41 - DEBUG - filelock -   Lock 140389248221344 acquired on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
Downloading:   0%|          | 0.00/916k [00:00<?, ?B/s]Downloading:   3%|▎         | 28.0k/916k [00:00<00:03, 283kB/s]Downloading:  20%|██        | 184k/916k [00:00<00:00, 1.03MB/s]Downloading:  90%|████████▉ | 824k/916k [00:00<00:00, 3.46MB/s]Downloading: 100%|██████████| 916k/916k [00:00<00:00, 3.02MB/s]
10/09/2022 07:01:42 - DEBUG - filelock -   Attempting to release lock 140389248221344 on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
10/09/2022 07:01:42 - DEBUG - filelock -   Lock 140389248221344 released on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
10/09/2022 07:01:42 - DEBUG - filelock -   Attempting to acquire lock 140389247028528 on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
10/09/2022 07:01:42 - DEBUG - filelock -   Lock 140389247028528 acquired on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
Downloading:   0%|          | 0.00/434k [00:00<?, ?B/s]Downloading:   6%|▋         | 28.0k/434k [00:00<00:01, 280kB/s]Downloading:  37%|███▋      | 160k/434k [00:00<00:00, 884kB/s] Downloading: 100%|██████████| 434k/434k [00:00<00:00, 1.70MB/s]
10/09/2022 07:01:43 - DEBUG - filelock -   Attempting to release lock 140389247028528 on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
10/09/2022 07:01:43 - DEBUG - filelock -   Lock 140389247028528 released on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
10/09/2022 07:01:43 - DEBUG - filelock -   Attempting to acquire lock 140389248706688 on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
10/09/2022 07:01:43 - DEBUG - filelock -   Lock 140389248706688 acquired on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]Downloading: 100%|██████████| 772/772 [00:00<00:00, 467kB/s]
10/09/2022 07:01:44 - DEBUG - filelock -   Attempting to release lock 140389248706688 on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
10/09/2022 07:01:44 - DEBUG - filelock -   Lock 140389248706688 released on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
10/09/2022 07:01:44 - DEBUG - filelock -   Attempting to acquire lock 140389248705680 on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
10/09/2022 07:01:44 - DEBUG - filelock -   Lock 140389248705680 acquired on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
Downloading:   0%|          | 0.00/1.09k [00:00<?, ?B/s]Downloading: 100%|██████████| 1.09k/1.09k [00:00<00:00, 931kB/s]
10/09/2022 07:01:44 - DEBUG - filelock -   Attempting to release lock 140389248705680 on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
10/09/2022 07:01:44 - DEBUG - filelock -   Lock 140389248705680 released on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
10/09/2022 07:01:45 - DEBUG - filelock -   Attempting to acquire lock 140389247028432 on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
10/09/2022 07:01:45 - DEBUG - filelock -   Lock 140389247028432 acquired on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
Downloading:   0%|          | 0.00/691 [00:00<?, ?B/s]Downloading: 100%|██████████| 691/691 [00:00<00:00, 422kB/s]
10/09/2022 07:01:45 - DEBUG - filelock -   Attempting to release lock 140389247028432 on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
10/09/2022 07:01:45 - DEBUG - filelock -   Lock 140389247028432 released on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
10/09/2022 07:01:46 - DEBUG - filelock -   Attempting to acquire lock 140389246889648 on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
10/09/2022 07:01:46 - DEBUG - filelock -   Lock 140389246889648 acquired on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
Downloading:   0%|          | 0.00/480M [00:00<?, ?B/s]Downloading:   1%|▏         | 6.21M/480M [00:00<00:07, 65.1MB/s]Downloading:   3%|▎         | 15.3M/480M [00:00<00:05, 82.6MB/s]Downloading:   5%|▌         | 25.3M/480M [00:00<00:05, 92.8MB/s]Downloading:   7%|▋         | 35.2M/480M [00:00<00:04, 97.1MB/s]Downloading:   9%|▉         | 45.0M/480M [00:00<00:04, 99.5MB/s]Downloading:  11%|█▏        | 55.0M/480M [00:00<00:04, 101MB/s] Downloading:  14%|█▎        | 64.9M/480M [00:00<00:04, 102MB/s]Downloading:  16%|█▌        | 74.8M/480M [00:00<00:04, 103MB/s]Downloading:  18%|█▊        | 84.6M/480M [00:00<00:04, 103MB/s]Downloading:  20%|█▉        | 94.7M/480M [00:01<00:03, 104MB/s]Downloading:  22%|██▏       | 105M/480M [00:01<00:03, 105MB/s] Downloading:  24%|██▍       | 116M/480M [00:01<00:03, 106MB/s]Downloading:  26%|██▌       | 126M/480M [00:01<00:03, 107MB/s]Downloading:  28%|██▊       | 136M/480M [00:01<00:03, 107MB/s]Downloading:  30%|███       | 146M/480M [00:01<00:03, 107MB/s]Downloading:  33%|███▎      | 157M/480M [00:01<00:03, 107MB/s]Downloading:  35%|███▍      | 167M/480M [00:01<00:03, 107MB/s]Downloading:  37%|███▋      | 177M/480M [00:01<00:02, 107MB/s]Downloading:  39%|███▉      | 187M/480M [00:01<00:02, 108MB/s]Downloading:  41%|████      | 198M/480M [00:02<00:02, 108MB/s]Downloading:  43%|████▎     | 208M/480M [00:02<00:02, 109MB/s]Downloading:  46%|████▌     | 219M/480M [00:02<00:02, 109MB/s]Downloading:  48%|████▊     | 229M/480M [00:02<00:02, 108MB/s]Downloading:  50%|████▉     | 239M/480M [00:02<00:02, 108MB/s]Downloading:  52%|█████▏    | 250M/480M [00:02<00:02, 108MB/s]Downloading:  54%|█████▍    | 260M/480M [00:02<00:02, 109MB/s]Downloading:  56%|█████▋    | 271M/480M [00:02<00:01, 111MB/s]Downloading:  59%|█████▊    | 282M/480M [00:02<00:01, 111MB/s]Downloading:  61%|██████    | 293M/480M [00:02<00:01, 110MB/s]Downloading:  63%|██████▎   | 303M/480M [00:03<00:01, 109MB/s]Downloading:  65%|██████▌   | 313M/480M [00:03<00:01, 109MB/s]Downloading:  67%|██████▋   | 324M/480M [00:03<00:01, 108MB/s]Downloading:  70%|██████▉   | 334M/480M [00:03<00:01, 109MB/s]Downloading:  72%|███████▏  | 345M/480M [00:03<00:01, 110MB/s]Downloading:  74%|███████▍  | 356M/480M [00:03<00:01, 110MB/s]Downloading:  76%|███████▋  | 366M/480M [00:03<00:01, 111MB/s]Downloading:  78%|███████▊  | 377M/480M [00:03<00:00, 111MB/s]Downloading:  81%|████████  | 388M/480M [00:03<00:00, 111MB/s]Downloading:  83%|████████▎ | 398M/480M [00:03<00:00, 110MB/s]Downloading:  85%|████████▌ | 409M/480M [00:04<00:00, 110MB/s]Downloading:  87%|████████▋ | 419M/480M [00:04<00:00, 110MB/s]Downloading:  90%|████████▉ | 430M/480M [00:04<00:00, 111MB/s]Downloading:  92%|█████████▏| 441M/480M [00:04<00:00, 111MB/s]Downloading:  94%|█████████▍| 451M/480M [00:04<00:00, 111MB/s]Downloading:  96%|█████████▌| 462M/480M [00:04<00:00, 111MB/s]Downloading:  98%|█████████▊| 473M/480M [00:04<00:00, 111MB/s]Downloading: 100%|██████████| 480M/480M [00:04<00:00, 107MB/s]
10/09/2022 07:01:51 - DEBUG - filelock -   Attempting to release lock 140389246889648 on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
10/09/2022 07:01:51 - DEBUG - filelock -   Lock 140389246889648 released on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
10/09/2022 07:01:52 - INFO - __main__ -   Training/evaluation parameters Namespace(code_length=256, codebase_file='dataset/CSN/python/codebase.jsonl', config_name='', debug=False, device=device(type='cuda'), do_F2_norm=False, do_eval=True, do_test=True, do_train=True, do_zero_shot=False, eval_batch_size=128, eval_data_file='dataset/CSN/python/valid.jsonl', freeze_bottom_k_layer_index=6, learning_rate=2e-05, max_grad_norm=1.0, model_name_or_path='microsoft/unixcoder-base', n_debug_samples=100, n_gpu=1, nl_length=128, num_train_epochs=10, output_dir='saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_6_layers/20221009070134', seed=123456, test_data_file='dataset/CSN/python/test.jsonl', tokenizer_name='', train_batch_size=128, train_data_file='dataset/CSN/python/train.jsonl', weight_decay=0.01)
10/09/2022 07:01:52 - INFO - __main__ -   +------------------------------------------------------------+--------------+---------+
| Layer Name                                                 | Output Shape | Param # |
+------------------------------------------------------------+--------------+---------+
| encoder.encoder.layer.6.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.6.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.6.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.6.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.6.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.6.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.6.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.6.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.6.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.6.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.6.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.6.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.7.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.7.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.7.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.7.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.7.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.7.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.7.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.7.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.7.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.7.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.7.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.7.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.8.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.8.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.8.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.8.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.8.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.8.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.8.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.8.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.8.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.8.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.8.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.8.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.9.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.9.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.9.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.9.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.9.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.9.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.9.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.9.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.9.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.9.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.9.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.9.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.10.attention.self.query.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.self.query.bias         |        [768] |     768 |
| encoder.encoder.layer.10.attention.self.key.weight         |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.self.key.bias           |        [768] |     768 |
| encoder.encoder.layer.10.attention.self.value.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.self.value.bias         |        [768] |     768 |
| encoder.encoder.layer.10.attention.output.dense.weight     |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.output.dense.bias       |        [768] |     768 |
| encoder.encoder.layer.10.attention.output.LayerNorm.weight |        [768] |     768 |
| encoder.encoder.layer.10.attention.output.LayerNorm.bias   |        [768] |     768 |
| encoder.encoder.layer.10.intermediate.dense.weight         |  [3072, 768] | 2359296 |
| encoder.encoder.layer.10.intermediate.dense.bias           |       [3072] |    3072 |
| encoder.encoder.layer.10.output.dense.weight               |  [768, 3072] | 2359296 |
| encoder.encoder.layer.10.output.dense.bias                 |        [768] |     768 |
| encoder.encoder.layer.10.output.LayerNorm.weight           |        [768] |     768 |
| encoder.encoder.layer.10.output.LayerNorm.bias             |        [768] |     768 |
| encoder.encoder.layer.11.attention.self.query.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.self.query.bias         |        [768] |     768 |
| encoder.encoder.layer.11.attention.self.key.weight         |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.self.key.bias           |        [768] |     768 |
| encoder.encoder.layer.11.attention.self.value.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.self.value.bias         |        [768] |     768 |
| encoder.encoder.layer.11.attention.output.dense.weight     |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.output.dense.bias       |        [768] |     768 |
| encoder.encoder.layer.11.attention.output.LayerNorm.weight |        [768] |     768 |
| encoder.encoder.layer.11.attention.output.LayerNorm.bias   |        [768] |     768 |
| encoder.encoder.layer.11.intermediate.dense.weight         |  [3072, 768] | 2359296 |
| encoder.encoder.layer.11.intermediate.dense.bias           |       [3072] |    3072 |
| encoder.encoder.layer.11.output.dense.weight               |  [768, 3072] | 2359296 |
| encoder.encoder.layer.11.output.dense.bias                 |        [768] |     768 |
| encoder.encoder.layer.11.output.LayerNorm.weight           |        [768] |     768 |
| encoder.encoder.layer.11.output.LayerNorm.bias             |        [768] |     768 |
| encoder.pooler.dense.weight                                |   [768, 768] |  589824 |
| encoder.pooler.dense.bias                                  |        [768] |     768 |
+------------------------------------------------------------+--------------+---------+
10/09/2022 07:05:22 - INFO - __main__ -   *** Example ***
10/09/2022 07:05:22 - INFO - __main__ -   idx: 0
10/09/2022 07:05:22 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_split', '_', 'phy', 'log', 'en', 'y', '_(', '_p', '_,', '_level', '_=', '_"', 's', '"', '_)', '_:', '_level', '_=', '_level', '_+', '_"__', '"', '_result', '_=', '_p', '_.', '_split', '_(', '_level', '_)', '_return', '_result', '_[', '_0', '_]', '_+', '_level', '_+', '_result', '_[', '_1', '_]', '_.', '_split', '_(', '_";"', '_)', '_[', '_0', '_]', '</s>']
10/09/2022 07:05:22 - INFO - __main__ -   code_ids: 0 6 2 729 5192 181 3258 896 386 207 400 428 2019 3144 385 437 201 120 743 545 3144 385 3144 513 12945 120 1046 385 428 746 5192 400 3144 743 483 1046 626 461 2406 513 3144 513 1046 626 524 2406 746 5192 400 29760 743 626 461 2406 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:05:22 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Return', '_either', '_the', '_full', '_or', '_truncated', '_version', '_of', '_a', '_Q', 'II', 'ME', '_-', '_formatted', '_taxonomy', '_string', '_.', '</s>']
10/09/2022 07:05:22 - INFO - __main__ -   nl_ids: 0 6 2 1675 4759 448 3662 872 19307 2229 595 434 1152 4300 1098 581 10440 29021 724 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:05:22 - INFO - __main__ -   *** Example ***
10/09/2022 07:05:22 - INFO - __main__ -   idx: 1
10/09/2022 07:05:22 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_ensure', '_', 'dir', '_(', '_d', '_)', '_:', '_if', '_not', '_os', '_.', '_path', '_.', '_exists', '_(', '_d', '_)', '_:', '_try', '_:', '_os', '_.', '_m', 'akedirs', '_(', '_d', '_)', '_except', '_OSError', '_as', '_oe', '_:', '_#', '_should', '_not', '_happen', '_with', '_os', '.', 'makedirs', '_#', '_ENOENT', ':', '_No', '_such', '_file', '_or', '_directory', '_if', '_os', '_.', '_errno', '_==', '_errno', '_.', '_ENOENT', '_:', '_msg', '_=', '_tw', 'dd', '_(', '_"""', 'One', '_or', '_more', '_directories', '_in', '_the', '_path', '_({})', '_do', '_not', '_exist', '.', '_If', 'Ċ', '__________________________', '_you', '_are', '_specifying', '_a', '_new', '_directory', '_for', '_output', ',', '_please', '_ensure', 'Ċ', '__________________________', '_all', '_other', '_directories', '_in', '_the', '_path', '_currently', '_exist', '."""', '_)', '_return', '_msg', '_.', '_format', '_(', '_d', '_)', '_else', '_:', '_msg', '_=', '_tw', 'dd', '_(', '_"""', 'An', '_error', '_occurred', '_trying', '_to', '_create', '_the', '_output', '_directory', 'Ċ', '__________________________', '_({})', '_with', '_message', ':', '_{}', '"""', '_)', '_return', '_msg', '_.', '_format', '_(', '_d', '_,', '_oe', '_.', '_strerror', '_)', '</s>']
10/09/2022 07:05:22 - INFO - __main__ -   code_ids: 0 6 2 729 6229 181 1282 400 480 743 545 462 800 2215 746 1391 746 4534 400 480 743 545 1568 545 2215 746 446 23328 400 480 743 3552 22934 880 44902 545 830 1570 800 7564 918 2215 132 24429 830 41059 144 4038 5632 1012 872 3456 462 2215 746 2341 550 2341 746 41059 545 2345 385 7916 443 400 1638 3533 872 2726 11613 488 448 1391 46072 1000 800 3040 132 1359 317 4584 2713 1147 15323 434 579 3456 563 1721 130 13874 6229 317 4584 1345 1946 11613 488 448 1391 6418 3040 6315 743 483 2345 746 2021 400 480 743 669 545 2345 385 7916 443 400 1638 1088 843 10058 11749 508 1738 448 1721 3456 317 4584 46072 918 1841 144 2334 3947 743 483 2345 746 2021 400 480 2019 44902 746 20115 743 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:05:22 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Check', '_to', '_make', '_sure', '_the', '_supplied', '_directory', '_path', '_does', '_not', '_exist', '_if', '_so', '_create', '_it', '_.', '_The', '_method', '_catch', 'es', '_OSError', '_exceptions', '_and', '_returns', '_a', '_desc', 'riptive', '_message', '_instead', '_of', '_re', '_-', '_raising', '_the', '_error', '_.', '</s>']
10/09/2022 07:05:22 - INFO - __main__ -   nl_ids: 0 6 2 1749 508 2002 3984 448 8813 3456 1391 2129 800 3040 462 1769 1738 835 746 1044 1454 2092 482 22934 12300 706 2060 434 2162 44105 1841 4488 595 479 581 47183 448 843 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:05:22 - INFO - __main__ -   *** Example ***
10/09/2022 07:05:22 - INFO - __main__ -   idx: 2
10/09/2022 07:05:22 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_file', '_', 'handle', '_(', '_fn', 'h', '_,', '_mode', '_=', '_"', 'r', 'U', '"', '_)', '_:', '_handle', '_=', '_None', '_if', '_isinstance', '_(', '_fn', 'h', '_,', '_file', '_)', '_:', '_if', '_fn', 'h', '_.', '_closed', '_:', '_raise', '_ValueError', '_(', '_"', 'Input', '_file', '_is', '_closed', '."', '_)', '_handle', '_=', '_fn', 'h', '_elif', '_isinstance', '_(', '_fn', 'h', '_,', '_str', '_)', '_:', '_handle', '_=', '_open', '_(', '_fn', 'h', '_,', '_mode', '_)', '_return', '_handle', '</s>']
10/09/2022 07:05:22 - INFO - __main__ -   code_ids: 0 6 2 729 1012 181 2133 400 4065 190 2019 2119 385 437 200 171 120 743 545 2384 385 1938 462 5408 400 4065 190 2019 1012 743 545 462 4065 190 746 8264 545 3085 6052 400 437 1834 1012 555 8264 3508 743 2384 385 4065 190 3625 5408 400 4065 190 2019 1113 743 545 2384 385 2717 400 4065 190 2019 2119 743 483 2384 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:05:22 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Takes', '_either', '_a', '_file', '_path', '_or', '_an', '_open', '_file', '_handle', '_checks', '_validity', '_and', '_returns', '_an', '_open', '_file', '_handle', '_or', '_raises', '_an', '_appropriate', '_Exception', '_.', '</s>']
10/09/2022 07:05:22 - INFO - __main__ -   nl_ids: 0 6 2 27408 4759 434 1012 1391 872 817 2717 1012 2384 7825 25911 706 2060 817 2717 1012 2384 872 23154 817 7900 2654 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:05:23 - INFO - __main__ -   ***** Running training *****
10/09/2022 07:05:23 - INFO - __main__ -     Num examples = 251820
10/09/2022 07:05:23 - INFO - __main__ -     Num Epochs = 10
10/09/2022 07:05:23 - INFO - __main__ -     Instantaneous batch size per GPU = 128
10/09/2022 07:05:23 - INFO - __main__ -     Total train batch size  = 128
10/09/2022 07:05:23 - INFO - __main__ -     Total optimization steps = 19680
10/09/2022 07:06:01 - INFO - __main__ -   epoch 0 step 100 loss 0.21786
10/09/2022 07:06:37 - INFO - __main__ -   epoch 0 step 200 loss 0.15753
10/09/2022 07:07:13 - INFO - __main__ -   epoch 0 step 300 loss 0.14461
10/09/2022 07:07:49 - INFO - __main__ -   epoch 0 step 400 loss 0.137
10/09/2022 07:08:25 - INFO - __main__ -   epoch 0 step 500 loss 0.14248
10/09/2022 07:09:02 - INFO - __main__ -   epoch 0 step 600 loss 0.1366
10/09/2022 07:09:38 - INFO - __main__ -   epoch 0 step 700 loss 0.13295
10/09/2022 07:10:14 - INFO - __main__ -   epoch 0 step 800 loss 0.12304
10/09/2022 07:10:50 - INFO - __main__ -   epoch 0 step 900 loss 0.12239
10/09/2022 07:11:26 - INFO - __main__ -   epoch 0 step 1000 loss 0.11466
10/09/2022 07:12:02 - INFO - __main__ -   epoch 0 step 1100 loss 0.1115
10/09/2022 07:12:38 - INFO - __main__ -   epoch 0 step 1200 loss 0.12483
10/09/2022 07:13:15 - INFO - __main__ -   epoch 0 step 1300 loss 0.10561
10/09/2022 07:13:51 - INFO - __main__ -   epoch 0 step 1400 loss 0.11753
10/09/2022 07:14:27 - INFO - __main__ -   epoch 0 step 1500 loss 0.11805
10/09/2022 07:15:03 - INFO - __main__ -   epoch 0 step 1600 loss 0.11234
10/09/2022 07:15:39 - INFO - __main__ -   epoch 0 step 1700 loss 0.11331
10/09/2022 07:16:15 - INFO - __main__ -   epoch 0 step 1800 loss 0.10913
10/09/2022 07:16:51 - INFO - __main__ -   epoch 0 step 1900 loss 0.11743
10/09/2022 07:17:54 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:17:54 - INFO - __main__ -     Num queries = 13914
10/09/2022 07:17:54 - INFO - __main__ -     Num codes = 43827
10/09/2022 07:17:54 - INFO - __main__ -     Batch size = 128
10/09/2022 07:19:45 - INFO - __main__ -     R@1 = 0.612
10/09/2022 07:19:45 - INFO - __main__ -     R@5 = 0.828
10/09/2022 07:19:45 - INFO - __main__ -     R@10 = 0.881
10/09/2022 07:19:45 - INFO - __main__ -     eval_mrr = 0.709
10/09/2022 07:19:45 - INFO - __main__ -     ********************
10/09/2022 07:19:45 - INFO - __main__ -     Best mrr:0.709
10/09/2022 07:19:45 - INFO - __main__ -     ********************
10/09/2022 07:19:54 - INFO - __main__ -   Saving model checkpoint to saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_6_layers/20221009070134/checkpoint-best-mrr/model.bin
10/09/2022 07:20:31 - INFO - __main__ -   epoch 1 step 100 loss 0.0967
10/09/2022 07:21:07 - INFO - __main__ -   epoch 1 step 200 loss 0.09035
10/09/2022 07:21:43 - INFO - __main__ -   epoch 1 step 300 loss 0.08637
10/09/2022 07:22:19 - INFO - __main__ -   epoch 1 step 400 loss 0.0873
10/09/2022 07:22:55 - INFO - __main__ -   epoch 1 step 500 loss 0.09007
10/09/2022 07:23:31 - INFO - __main__ -   epoch 1 step 600 loss 0.09094
10/09/2022 07:24:07 - INFO - __main__ -   epoch 1 step 700 loss 0.08396
10/09/2022 07:24:44 - INFO - __main__ -   epoch 1 step 800 loss 0.09536
10/09/2022 07:25:20 - INFO - __main__ -   epoch 1 step 900 loss 0.09396
10/09/2022 07:25:56 - INFO - __main__ -   epoch 1 step 1000 loss 0.09115
10/09/2022 07:26:32 - INFO - __main__ -   epoch 1 step 1100 loss 0.09246
10/09/2022 07:27:08 - INFO - __main__ -   epoch 1 step 1200 loss 0.0866
10/09/2022 07:27:44 - INFO - __main__ -   epoch 1 step 1300 loss 0.08846
10/09/2022 07:28:20 - INFO - __main__ -   epoch 1 step 1400 loss 0.08749
10/09/2022 07:28:56 - INFO - __main__ -   epoch 1 step 1500 loss 0.08377
10/09/2022 07:29:33 - INFO - __main__ -   epoch 1 step 1600 loss 0.08571
10/09/2022 07:30:09 - INFO - __main__ -   epoch 1 step 1700 loss 0.08808
10/09/2022 07:30:45 - INFO - __main__ -   epoch 1 step 1800 loss 0.08854
10/09/2022 07:31:21 - INFO - __main__ -   epoch 1 step 1900 loss 0.08547
10/09/2022 07:32:19 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:32:19 - INFO - __main__ -     Num queries = 13914
10/09/2022 07:32:19 - INFO - __main__ -     Num codes = 43827
10/09/2022 07:32:19 - INFO - __main__ -     Batch size = 128
10/09/2022 07:34:10 - INFO - __main__ -     R@1 = 0.615
10/09/2022 07:34:10 - INFO - __main__ -     R@5 = 0.835
10/09/2022 07:34:10 - INFO - __main__ -     R@10 = 0.885
10/09/2022 07:34:10 - INFO - __main__ -     eval_mrr = 0.714
10/09/2022 07:34:10 - INFO - __main__ -     ********************
10/09/2022 07:34:10 - INFO - __main__ -     Best mrr:0.714
10/09/2022 07:34:10 - INFO - __main__ -     ********************
10/09/2022 07:34:15 - INFO - __main__ -   Saving model checkpoint to saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_6_layers/20221009070134/checkpoint-best-mrr/model.bin
10/09/2022 07:34:52 - INFO - __main__ -   epoch 2 step 100 loss 0.07357
10/09/2022 07:35:28 - INFO - __main__ -   epoch 2 step 200 loss 0.06546
10/09/2022 07:36:05 - INFO - __main__ -   epoch 2 step 300 loss 0.06735
10/09/2022 07:36:41 - INFO - __main__ -   epoch 2 step 400 loss 0.06671
10/09/2022 07:37:17 - INFO - __main__ -   epoch 2 step 500 loss 0.07137
10/09/2022 07:37:53 - INFO - __main__ -   epoch 2 step 600 loss 0.07047
10/09/2022 07:38:29 - INFO - __main__ -   epoch 2 step 700 loss 0.06863
10/09/2022 07:39:05 - INFO - __main__ -   epoch 2 step 800 loss 0.0656
10/09/2022 07:39:42 - INFO - __main__ -   epoch 2 step 900 loss 0.0758
10/09/2022 07:40:18 - INFO - __main__ -   epoch 2 step 1000 loss 0.06989
10/09/2022 07:40:54 - INFO - __main__ -   epoch 2 step 1100 loss 0.07307
10/09/2022 07:41:30 - INFO - __main__ -   epoch 2 step 1200 loss 0.07141
10/09/2022 07:42:06 - INFO - __main__ -   epoch 2 step 1300 loss 0.07452
10/09/2022 07:42:42 - INFO - __main__ -   epoch 2 step 1400 loss 0.07423
10/09/2022 07:43:19 - INFO - __main__ -   epoch 2 step 1500 loss 0.06962
10/09/2022 07:43:55 - INFO - __main__ -   epoch 2 step 1600 loss 0.0738
10/09/2022 07:44:31 - INFO - __main__ -   epoch 2 step 1700 loss 0.0709
10/09/2022 07:45:07 - INFO - __main__ -   epoch 2 step 1800 loss 0.07074
10/09/2022 07:45:43 - INFO - __main__ -   epoch 2 step 1900 loss 0.0725
10/09/2022 07:46:41 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:46:41 - INFO - __main__ -     Num queries = 13914
10/09/2022 07:46:41 - INFO - __main__ -     Num codes = 43827
10/09/2022 07:46:41 - INFO - __main__ -     Batch size = 128
10/09/2022 07:48:33 - INFO - __main__ -     R@1 = 0.616
10/09/2022 07:48:33 - INFO - __main__ -     R@5 = 0.834
10/09/2022 07:48:33 - INFO - __main__ -     R@10 = 0.885
10/09/2022 07:48:33 - INFO - __main__ -     eval_mrr = 0.714
10/09/2022 07:49:10 - INFO - __main__ -   epoch 3 step 100 loss 0.06295
10/09/2022 07:49:46 - INFO - __main__ -   epoch 3 step 200 loss 0.05419
10/09/2022 07:50:22 - INFO - __main__ -   epoch 3 step 300 loss 0.05985
10/09/2022 07:50:59 - INFO - __main__ -   epoch 3 step 400 loss 0.05814
10/09/2022 07:51:35 - INFO - __main__ -   epoch 3 step 500 loss 0.06043
10/09/2022 07:52:11 - INFO - __main__ -   epoch 3 step 600 loss 0.05727
10/09/2022 07:52:47 - INFO - __main__ -   epoch 3 step 700 loss 0.05964
10/09/2022 07:53:23 - INFO - __main__ -   epoch 3 step 800 loss 0.05874
10/09/2022 07:53:59 - INFO - __main__ -   epoch 3 step 900 loss 0.05992
10/09/2022 07:54:35 - INFO - __main__ -   epoch 3 step 1000 loss 0.05835
10/09/2022 07:55:12 - INFO - __main__ -   epoch 3 step 1100 loss 0.05523
10/09/2022 07:55:48 - INFO - __main__ -   epoch 3 step 1200 loss 0.05865
10/09/2022 07:56:24 - INFO - __main__ -   epoch 3 step 1300 loss 0.0588
10/09/2022 07:57:00 - INFO - __main__ -   epoch 3 step 1400 loss 0.0577
10/09/2022 07:57:36 - INFO - __main__ -   epoch 3 step 1500 loss 0.05216
10/09/2022 07:58:12 - INFO - __main__ -   epoch 3 step 1600 loss 0.05889
10/09/2022 07:58:48 - INFO - __main__ -   epoch 3 step 1700 loss 0.05669
10/09/2022 07:59:25 - INFO - __main__ -   epoch 3 step 1800 loss 0.05687
10/09/2022 08:00:01 - INFO - __main__ -   epoch 3 step 1900 loss 0.06187
10/09/2022 08:00:59 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 08:00:59 - INFO - __main__ -     Num queries = 13914
10/09/2022 08:00:59 - INFO - __main__ -     Num codes = 43827
10/09/2022 08:00:59 - INFO - __main__ -     Batch size = 128
10/09/2022 08:02:50 - INFO - __main__ -     R@1 = 0.616
10/09/2022 08:02:50 - INFO - __main__ -     R@5 = 0.834
10/09/2022 08:02:50 - INFO - __main__ -     R@10 = 0.884
10/09/2022 08:02:50 - INFO - __main__ -     eval_mrr = 0.713
10/09/2022 08:03:26 - INFO - __main__ -   epoch 4 step 100 loss 0.04999
10/09/2022 08:04:03 - INFO - __main__ -   epoch 4 step 200 loss 0.0461
10/09/2022 08:04:39 - INFO - __main__ -   epoch 4 step 300 loss 0.05136
10/09/2022 08:05:15 - INFO - __main__ -   epoch 4 step 400 loss 0.04967
10/09/2022 08:05:51 - INFO - __main__ -   epoch 4 step 500 loss 0.04922
10/09/2022 08:06:27 - INFO - __main__ -   epoch 4 step 600 loss 0.05035
10/09/2022 08:07:03 - INFO - __main__ -   epoch 4 step 700 loss 0.04928
10/09/2022 08:07:40 - INFO - __main__ -   epoch 4 step 800 loss 0.04856
10/09/2022 08:08:16 - INFO - __main__ -   epoch 4 step 900 loss 0.04849
10/09/2022 08:08:52 - INFO - __main__ -   epoch 4 step 1000 loss 0.04883
10/09/2022 08:09:28 - INFO - __main__ -   epoch 4 step 1100 loss 0.0457
10/09/2022 08:10:04 - INFO - __main__ -   epoch 4 step 1200 loss 0.0492
10/09/2022 08:10:40 - INFO - __main__ -   epoch 4 step 1300 loss 0.0488
10/09/2022 08:11:16 - INFO - __main__ -   epoch 4 step 1400 loss 0.04757
10/09/2022 08:11:52 - INFO - __main__ -   epoch 4 step 1500 loss 0.05044
10/09/2022 08:12:29 - INFO - __main__ -   epoch 4 step 1600 loss 0.04955
10/09/2022 08:13:05 - INFO - __main__ -   epoch 4 step 1700 loss 0.04774
10/09/2022 08:13:41 - INFO - __main__ -   epoch 4 step 1800 loss 0.05161
10/09/2022 08:14:17 - INFO - __main__ -   epoch 4 step 1900 loss 0.05022
10/09/2022 08:15:16 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 08:15:16 - INFO - __main__ -     Num queries = 13914
10/09/2022 08:15:16 - INFO - __main__ -     Num codes = 43827
10/09/2022 08:15:16 - INFO - __main__ -     Batch size = 128
10/09/2022 08:17:06 - INFO - __main__ -     R@1 = 0.617
10/09/2022 08:17:06 - INFO - __main__ -     R@5 = 0.836
10/09/2022 08:17:06 - INFO - __main__ -     R@10 = 0.886
10/09/2022 08:17:06 - INFO - __main__ -     eval_mrr = 0.714
10/09/2022 08:17:43 - INFO - __main__ -   epoch 5 step 100 loss 0.04313
10/09/2022 08:18:19 - INFO - __main__ -   epoch 5 step 200 loss 0.04208
10/09/2022 08:18:55 - INFO - __main__ -   epoch 5 step 300 loss 0.04251
10/09/2022 08:19:32 - INFO - __main__ -   epoch 5 step 400 loss 0.04087
10/09/2022 08:20:08 - INFO - __main__ -   epoch 5 step 500 loss 0.04062
10/09/2022 08:20:44 - INFO - __main__ -   epoch 5 step 600 loss 0.04217
10/09/2022 08:21:20 - INFO - __main__ -   epoch 5 step 700 loss 0.03995
10/09/2022 08:21:56 - INFO - __main__ -   epoch 5 step 800 loss 0.04473
10/09/2022 08:22:32 - INFO - __main__ -   epoch 5 step 900 loss 0.0436
10/09/2022 08:23:09 - INFO - __main__ -   epoch 5 step 1000 loss 0.04157
10/09/2022 08:23:45 - INFO - __main__ -   epoch 5 step 1100 loss 0.04104
10/09/2022 08:24:21 - INFO - __main__ -   epoch 5 step 1200 loss 0.03926
10/09/2022 08:24:57 - INFO - __main__ -   epoch 5 step 1300 loss 0.04313
10/09/2022 08:25:33 - INFO - __main__ -   epoch 5 step 1400 loss 0.04138
10/09/2022 08:26:09 - INFO - __main__ -   epoch 5 step 1500 loss 0.04396
10/09/2022 08:26:46 - INFO - __main__ -   epoch 5 step 1600 loss 0.04489
10/09/2022 08:27:22 - INFO - __main__ -   epoch 5 step 1700 loss 0.04058
10/09/2022 08:27:58 - INFO - __main__ -   epoch 5 step 1800 loss 0.04594
10/09/2022 08:28:34 - INFO - __main__ -   epoch 5 step 1900 loss 0.04374
10/09/2022 08:29:32 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 08:29:32 - INFO - __main__ -     Num queries = 13914
10/09/2022 08:29:32 - INFO - __main__ -     Num codes = 43827
10/09/2022 08:29:32 - INFO - __main__ -     Batch size = 128
10/09/2022 08:31:24 - INFO - __main__ -     R@1 = 0.617
10/09/2022 08:31:24 - INFO - __main__ -     R@5 = 0.835
10/09/2022 08:31:24 - INFO - __main__ -     R@10 = 0.885
10/09/2022 08:31:24 - INFO - __main__ -     eval_mrr = 0.714
10/09/2022 08:32:01 - INFO - __main__ -   epoch 6 step 100 loss 0.03947
10/09/2022 08:32:37 - INFO - __main__ -   epoch 6 step 200 loss 0.03943
10/09/2022 08:33:13 - INFO - __main__ -   epoch 6 step 300 loss 0.03658
10/09/2022 08:33:49 - INFO - __main__ -   epoch 6 step 400 loss 0.03554
10/09/2022 08:34:25 - INFO - __main__ -   epoch 6 step 500 loss 0.03712
10/09/2022 08:35:02 - INFO - __main__ -   epoch 6 step 600 loss 0.03784
10/09/2022 08:35:38 - INFO - __main__ -   epoch 6 step 700 loss 0.0355
10/09/2022 08:36:14 - INFO - __main__ -   epoch 6 step 800 loss 0.03648
10/09/2022 08:36:50 - INFO - __main__ -   epoch 6 step 900 loss 0.03694
10/09/2022 08:37:26 - INFO - __main__ -   epoch 6 step 1000 loss 0.03689
10/09/2022 08:38:02 - INFO - __main__ -   epoch 6 step 1100 loss 0.03706
10/09/2022 08:38:39 - INFO - __main__ -   epoch 6 step 1200 loss 0.03687
10/09/2022 08:39:15 - INFO - __main__ -   epoch 6 step 1300 loss 0.03539
10/09/2022 08:39:51 - INFO - __main__ -   epoch 6 step 1400 loss 0.03969
10/09/2022 08:40:27 - INFO - __main__ -   epoch 6 step 1500 loss 0.03874
10/09/2022 08:41:03 - INFO - __main__ -   epoch 6 step 1600 loss 0.0374
10/09/2022 08:41:39 - INFO - __main__ -   epoch 6 step 1700 loss 0.03761
10/09/2022 08:42:16 - INFO - __main__ -   epoch 6 step 1800 loss 0.03808
10/09/2022 08:42:52 - INFO - __main__ -   epoch 6 step 1900 loss 0.0381
10/09/2022 08:43:50 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 08:43:50 - INFO - __main__ -     Num queries = 13914
10/09/2022 08:43:50 - INFO - __main__ -     Num codes = 43827
10/09/2022 08:43:50 - INFO - __main__ -     Batch size = 128
10/09/2022 08:45:42 - INFO - __main__ -     R@1 = 0.616
10/09/2022 08:45:42 - INFO - __main__ -     R@5 = 0.837
10/09/2022 08:45:42 - INFO - __main__ -     R@10 = 0.886
10/09/2022 08:45:42 - INFO - __main__ -     eval_mrr = 0.714
10/09/2022 08:46:19 - INFO - __main__ -   epoch 7 step 100 loss 0.03537
10/09/2022 08:46:55 - INFO - __main__ -   epoch 7 step 200 loss 0.03487
10/09/2022 08:47:31 - INFO - __main__ -   epoch 7 step 300 loss 0.03377
10/09/2022 08:48:07 - INFO - __main__ -   epoch 7 step 400 loss 0.03558
10/09/2022 08:48:43 - INFO - __main__ -   epoch 7 step 500 loss 0.03286
10/09/2022 08:49:19 - INFO - __main__ -   epoch 7 step 600 loss 0.03568
10/09/2022 08:49:56 - INFO - __main__ -   epoch 7 step 700 loss 0.03462
10/09/2022 08:50:32 - INFO - __main__ -   epoch 7 step 800 loss 0.03296
10/09/2022 08:51:08 - INFO - __main__ -   epoch 7 step 900 loss 0.03375
10/09/2022 08:51:44 - INFO - __main__ -   epoch 7 step 1000 loss 0.03366
10/09/2022 08:52:20 - INFO - __main__ -   epoch 7 step 1100 loss 0.03603
10/09/2022 08:52:56 - INFO - __main__ -   epoch 7 step 1200 loss 0.03408
10/09/2022 08:53:33 - INFO - __main__ -   epoch 7 step 1300 loss 0.03462
10/09/2022 08:54:09 - INFO - __main__ -   epoch 7 step 1400 loss 0.03585
10/09/2022 08:54:45 - INFO - __main__ -   epoch 7 step 1500 loss 0.03199
10/09/2022 08:55:21 - INFO - __main__ -   epoch 7 step 1600 loss 0.03526
10/09/2022 08:55:57 - INFO - __main__ -   epoch 7 step 1700 loss 0.0327
10/09/2022 08:56:33 - INFO - __main__ -   epoch 7 step 1800 loss 0.03534
10/09/2022 08:57:10 - INFO - __main__ -   epoch 7 step 1900 loss 0.03355
10/09/2022 08:58:06 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 08:58:06 - INFO - __main__ -     Num queries = 13914
10/09/2022 08:58:06 - INFO - __main__ -     Num codes = 43827
10/09/2022 08:58:06 - INFO - __main__ -     Batch size = 128
10/09/2022 08:59:57 - INFO - __main__ -     R@1 = 0.615
10/09/2022 08:59:57 - INFO - __main__ -     R@5 = 0.836
10/09/2022 08:59:57 - INFO - __main__ -     R@10 = 0.886
10/09/2022 08:59:57 - INFO - __main__ -     eval_mrr = 0.714
10/09/2022 09:00:34 - INFO - __main__ -   epoch 8 step 100 loss 0.0321
10/09/2022 09:01:10 - INFO - __main__ -   epoch 8 step 200 loss 0.03065
10/09/2022 09:01:47 - INFO - __main__ -   epoch 8 step 300 loss 0.03217
10/09/2022 09:02:23 - INFO - __main__ -   epoch 8 step 400 loss 0.03079
10/09/2022 09:02:59 - INFO - __main__ -   epoch 8 step 500 loss 0.03158
10/09/2022 09:03:35 - INFO - __main__ -   epoch 8 step 600 loss 0.03399
10/09/2022 09:04:11 - INFO - __main__ -   epoch 8 step 700 loss 0.03045
10/09/2022 09:04:47 - INFO - __main__ -   epoch 8 step 800 loss 0.03246
10/09/2022 09:05:24 - INFO - __main__ -   epoch 8 step 900 loss 0.03236
10/09/2022 09:06:00 - INFO - __main__ -   epoch 8 step 1000 loss 0.03149
10/09/2022 09:06:36 - INFO - __main__ -   epoch 8 step 1100 loss 0.03266
10/09/2022 09:07:12 - INFO - __main__ -   epoch 8 step 1200 loss 0.03386
10/09/2022 09:07:48 - INFO - __main__ -   epoch 8 step 1300 loss 0.03293
10/09/2022 09:08:24 - INFO - __main__ -   epoch 8 step 1400 loss 0.0319
10/09/2022 09:09:01 - INFO - __main__ -   epoch 8 step 1500 loss 0.03396
10/09/2022 09:09:37 - INFO - __main__ -   epoch 8 step 1600 loss 0.03033
10/09/2022 09:10:13 - INFO - __main__ -   epoch 8 step 1700 loss 0.03204
10/09/2022 09:10:49 - INFO - __main__ -   epoch 8 step 1800 loss 0.03244
10/09/2022 09:11:25 - INFO - __main__ -   epoch 8 step 1900 loss 0.03375
10/09/2022 09:12:25 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 09:12:25 - INFO - __main__ -     Num queries = 13914
10/09/2022 09:12:25 - INFO - __main__ -     Num codes = 43827
10/09/2022 09:12:25 - INFO - __main__ -     Batch size = 128
10/09/2022 09:14:18 - INFO - __main__ -     R@1 = 0.616
10/09/2022 09:14:18 - INFO - __main__ -     R@5 = 0.838
10/09/2022 09:14:18 - INFO - __main__ -     R@10 = 0.888
10/09/2022 09:14:18 - INFO - __main__ -     eval_mrr = 0.715
10/09/2022 09:14:18 - INFO - __main__ -     ********************
10/09/2022 09:14:18 - INFO - __main__ -     Best mrr:0.715
10/09/2022 09:14:18 - INFO - __main__ -     ********************
10/09/2022 09:14:27 - INFO - __main__ -   Saving model checkpoint to saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_6_layers/20221009070134/checkpoint-best-mrr/model.bin
10/09/2022 09:15:04 - INFO - __main__ -   epoch 9 step 100 loss 0.03327
10/09/2022 09:15:40 - INFO - __main__ -   epoch 9 step 200 loss 0.03113
10/09/2022 09:16:16 - INFO - __main__ -   epoch 9 step 300 loss 0.03298
10/09/2022 09:16:52 - INFO - __main__ -   epoch 9 step 400 loss 0.03123
10/09/2022 09:17:29 - INFO - __main__ -   epoch 9 step 500 loss 0.03122
10/09/2022 09:18:05 - INFO - __main__ -   epoch 9 step 600 loss 0.02976
10/09/2022 09:18:41 - INFO - __main__ -   epoch 9 step 700 loss 0.02977
10/09/2022 09:19:17 - INFO - __main__ -   epoch 9 step 800 loss 0.0318
10/09/2022 09:19:53 - INFO - __main__ -   epoch 9 step 900 loss 0.03128
10/09/2022 09:20:30 - INFO - __main__ -   epoch 9 step 1000 loss 0.03056
10/09/2022 09:21:06 - INFO - __main__ -   epoch 9 step 1100 loss 0.02938
10/09/2022 09:21:42 - INFO - __main__ -   epoch 9 step 1200 loss 0.03095
10/09/2022 09:22:18 - INFO - __main__ -   epoch 9 step 1300 loss 0.03057
10/09/2022 09:22:54 - INFO - __main__ -   epoch 9 step 1400 loss 0.02996
10/09/2022 09:23:30 - INFO - __main__ -   epoch 9 step 1500 loss 0.03077
10/09/2022 09:24:07 - INFO - __main__ -   epoch 9 step 1600 loss 0.0325
10/09/2022 09:24:43 - INFO - __main__ -   epoch 9 step 1700 loss 0.02945
10/09/2022 09:25:19 - INFO - __main__ -   epoch 9 step 1800 loss 0.03018
10/09/2022 09:25:55 - INFO - __main__ -   epoch 9 step 1900 loss 0.02956
10/09/2022 09:26:53 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 09:26:53 - INFO - __main__ -     Num queries = 13914
10/09/2022 09:26:53 - INFO - __main__ -     Num codes = 43827
10/09/2022 09:26:53 - INFO - __main__ -     Batch size = 128
10/09/2022 09:28:44 - INFO - __main__ -     R@1 = 0.617
10/09/2022 09:28:44 - INFO - __main__ -     R@5 = 0.838
10/09/2022 09:28:44 - INFO - __main__ -     R@10 = 0.887
10/09/2022 09:28:44 - INFO - __main__ -     eval_mrr = 0.715
10/09/2022 09:29:18 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 09:29:18 - INFO - __main__ -     Num queries = 13914
10/09/2022 09:29:18 - INFO - __main__ -     Num codes = 43827
10/09/2022 09:29:18 - INFO - __main__ -     Batch size = 128
10/09/2022 09:31:11 - INFO - __main__ -   ***** Eval results *****
10/09/2022 09:31:11 - INFO - __main__ -     R@1 = 0.616
10/09/2022 09:31:11 - INFO - __main__ -     R@10 = 0.888
10/09/2022 09:31:11 - INFO - __main__ -     R@5 = 0.838
10/09/2022 09:31:11 - INFO - __main__ -     eval_mrr = 0.715
10/09/2022 09:31:45 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 09:31:45 - INFO - __main__ -     Num queries = 14918
10/09/2022 09:31:45 - INFO - __main__ -     Num codes = 43827
10/09/2022 09:31:45 - INFO - __main__ -     Batch size = 128
10/09/2022 09:33:43 - INFO - __main__ -   ***** Eval results *****
10/09/2022 09:33:43 - INFO - __main__ -     R@1 = 0.629
10/09/2022 09:33:43 - INFO - __main__ -     R@10 = 0.896
10/09/2022 09:33:43 - INFO - __main__ -     R@5 = 0.848
10/09/2022 09:33:43 - INFO - __main__ -     eval_mrr = 0.727
10/09/2022 09:33:43 - INFO - utils -   saved dataset in saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_6_layers/20221009070134/result.jsonl
