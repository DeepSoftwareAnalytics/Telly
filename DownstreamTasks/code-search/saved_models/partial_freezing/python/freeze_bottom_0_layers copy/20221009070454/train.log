10/09/2022 07:05:00 - INFO - __main__ -   device: cuda, n_gpu: 1
10/09/2022 07:05:01 - DEBUG - filelock -   Attempting to acquire lock 140335607209168 on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
10/09/2022 07:05:01 - DEBUG - filelock -   Lock 140335607209168 acquired on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
Downloading:   0%|          | 0.00/916k [00:00<?, ?B/s]Downloading:   3%|▎         | 28.0k/916k [00:00<00:03, 270kB/s]Downloading:  19%|█▉        | 172k/916k [00:00<00:00, 914kB/s] Downloading:  76%|███████▋  | 700k/916k [00:00<00:00, 2.77MB/s]Downloading: 100%|██████████| 916k/916k [00:00<00:00, 2.86MB/s]
10/09/2022 07:05:02 - DEBUG - filelock -   Attempting to release lock 140335607209168 on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
10/09/2022 07:05:02 - DEBUG - filelock -   Lock 140335607209168 released on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
10/09/2022 07:05:02 - DEBUG - filelock -   Attempting to acquire lock 140335607070144 on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
10/09/2022 07:05:02 - DEBUG - filelock -   Lock 140335607070144 acquired on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
Downloading:   0%|          | 0.00/434k [00:00<?, ?B/s]Downloading:   9%|▉         | 40.0k/434k [00:00<00:01, 384kB/s]Downloading:  41%|████▏     | 180k/434k [00:00<00:00, 938kB/s] Downloading: 100%|██████████| 434k/434k [00:00<00:00, 1.64MB/s]
10/09/2022 07:05:02 - DEBUG - filelock -   Attempting to release lock 140335607070144 on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
10/09/2022 07:05:02 - DEBUG - filelock -   Lock 140335607070144 released on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
10/09/2022 07:05:03 - DEBUG - filelock -   Attempting to acquire lock 140335607209168 on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
10/09/2022 07:05:03 - DEBUG - filelock -   Lock 140335607209168 acquired on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]Downloading: 100%|██████████| 772/772 [00:00<00:00, 789kB/s]
10/09/2022 07:05:03 - DEBUG - filelock -   Attempting to release lock 140335607209168 on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
10/09/2022 07:05:03 - DEBUG - filelock -   Lock 140335607209168 released on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
10/09/2022 07:05:03 - DEBUG - filelock -   Attempting to acquire lock 140335608890560 on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
10/09/2022 07:05:03 - DEBUG - filelock -   Lock 140335608890560 acquired on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
Downloading:   0%|          | 0.00/1.09k [00:00<?, ?B/s]Downloading: 100%|██████████| 1.09k/1.09k [00:00<00:00, 1.10MB/s]
10/09/2022 07:05:04 - DEBUG - filelock -   Attempting to release lock 140335608890560 on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
10/09/2022 07:05:04 - DEBUG - filelock -   Lock 140335608890560 released on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
10/09/2022 07:05:04 - DEBUG - filelock -   Attempting to acquire lock 140335607208928 on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
10/09/2022 07:05:04 - DEBUG - filelock -   Lock 140335607208928 acquired on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
Downloading:   0%|          | 0.00/691 [00:00<?, ?B/s]Downloading: 100%|██████████| 691/691 [00:00<00:00, 414kB/s]
10/09/2022 07:05:05 - DEBUG - filelock -   Attempting to release lock 140335607208928 on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
10/09/2022 07:05:05 - DEBUG - filelock -   Lock 140335607208928 released on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
10/09/2022 07:05:05 - DEBUG - filelock -   Attempting to acquire lock 140335607069808 on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
10/09/2022 07:05:05 - DEBUG - filelock -   Lock 140335607069808 acquired on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
Downloading:   0%|          | 0.00/480M [00:00<?, ?B/s]Downloading:   1%|▏         | 7.06M/480M [00:00<00:06, 74.0MB/s]Downloading:   4%|▎         | 17.7M/480M [00:00<00:05, 96.2MB/s]Downloading:   6%|▌         | 28.4M/480M [00:00<00:04, 103MB/s] Downloading:   8%|▊         | 39.2M/480M [00:00<00:04, 107MB/s]Downloading:  10%|█         | 49.8M/480M [00:00<00:04, 109MB/s]Downloading:  13%|█▎        | 60.4M/480M [00:00<00:04, 109MB/s]Downloading:  15%|█▍        | 71.2M/480M [00:00<00:03, 111MB/s]Downloading:  17%|█▋        | 81.9M/480M [00:00<00:03, 111MB/s]Downloading:  19%|█▉        | 92.6M/480M [00:00<00:03, 111MB/s]Downloading:  22%|██▏       | 104M/480M [00:01<00:03, 112MB/s] Downloading:  24%|██▍       | 114M/480M [00:01<00:03, 113MB/s]Downloading:  26%|██▌       | 125M/480M [00:01<00:03, 113MB/s]Downloading:  28%|██▊       | 136M/480M [00:01<00:03, 113MB/s]Downloading:  31%|███       | 147M/480M [00:01<00:03, 112MB/s]Downloading:  33%|███▎      | 158M/480M [00:01<00:03, 112MB/s]Downloading:  35%|███▌      | 168M/480M [00:01<00:02, 112MB/s]Downloading:  37%|███▋      | 179M/480M [00:01<00:02, 112MB/s]Downloading:  39%|███▉      | 190M/480M [00:01<00:02, 111MB/s]Downloading:  42%|████▏     | 200M/480M [00:01<00:02, 112MB/s]Downloading:  44%|████▍     | 211M/480M [00:02<00:02, 112MB/s]Downloading:  46%|████▋     | 222M/480M [00:02<00:02, 113MB/s]Downloading:  49%|████▊     | 233M/480M [00:02<00:02, 114MB/s]Downloading:  51%|█████     | 244M/480M [00:02<00:02, 113MB/s]Downloading:  53%|█████▎    | 255M/480M [00:02<00:02, 112MB/s]Downloading:  55%|█████▌    | 266M/480M [00:02<00:02, 112MB/s]Downloading:  58%|█████▊    | 276M/480M [00:02<00:01, 112MB/s]Downloading:  60%|█████▉    | 287M/480M [00:02<00:01, 112MB/s]Downloading:  62%|██████▏   | 298M/480M [00:02<00:01, 112MB/s]Downloading:  64%|██████▍   | 308M/480M [00:02<00:01, 111MB/s]Downloading:  66%|██████▋   | 319M/480M [00:03<00:01, 112MB/s]Downloading:  69%|██████▊   | 330M/480M [00:03<00:01, 112MB/s]Downloading:  71%|███████   | 341M/480M [00:03<00:01, 112MB/s]Downloading:  73%|███████▎  | 352M/480M [00:03<00:01, 112MB/s]Downloading:  75%|███████▌  | 363M/480M [00:03<00:01, 113MB/s]Downloading:  78%|███████▊  | 373M/480M [00:03<00:00, 113MB/s]Downloading:  80%|███████▉  | 384M/480M [00:03<00:00, 113MB/s]Downloading:  82%|████████▏ | 395M/480M [00:03<00:00, 112MB/s]Downloading:  84%|████████▍ | 406M/480M [00:03<00:00, 112MB/s]Downloading:  87%|████████▋ | 417M/480M [00:03<00:00, 113MB/s]Downloading:  89%|████████▉ | 428M/480M [00:04<00:00, 114MB/s]Downloading:  91%|█████████▏| 439M/480M [00:04<00:00, 115MB/s]Downloading:  94%|█████████▎| 450M/480M [00:04<00:00, 115MB/s]Downloading:  96%|█████████▌| 461M/480M [00:04<00:00, 115MB/s]Downloading:  98%|█████████▊| 472M/480M [00:04<00:00, 114MB/s]Downloading: 100%|██████████| 480M/480M [00:04<00:00, 112MB/s]
10/09/2022 07:05:10 - DEBUG - filelock -   Attempting to release lock 140335607069808 on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
10/09/2022 07:05:10 - DEBUG - filelock -   Lock 140335607069808 released on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
10/09/2022 07:05:12 - INFO - __main__ -   Training/evaluation parameters Namespace(code_length=256, codebase_file='dataset/CSN/python/codebase.jsonl', config_name='', debug=False, device=device(type='cuda'), do_F2_norm=False, do_eval=True, do_test=True, do_train=True, do_zero_shot=False, eval_batch_size=128, eval_data_file='dataset/CSN/python/valid.jsonl', freeze_bottom_k_layer_index=0, learning_rate=2e-05, max_grad_norm=1.0, model_name_or_path='microsoft/unixcoder-base', n_debug_samples=100, n_gpu=1, nl_length=128, num_train_epochs=10, output_dir='saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_0_layers/20221009070454', seed=123456, test_data_file='dataset/CSN/python/test.jsonl', tokenizer_name='', train_batch_size=128, train_data_file='dataset/CSN/python/train.jsonl', weight_decay=0.01)
10/09/2022 07:05:12 - INFO - __main__ -   +------------------------------------------------------------+--------------+---------+
| Layer Name                                                 | Output Shape | Param # |
+------------------------------------------------------------+--------------+---------+
| encoder.encoder.layer.0.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.0.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.0.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.0.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.0.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.0.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.0.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.0.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.0.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.0.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.0.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.0.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.0.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.0.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.0.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.0.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.1.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.1.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.1.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.1.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.1.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.1.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.1.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.1.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.1.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.1.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.1.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.1.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.1.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.1.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.1.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.1.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.2.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.2.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.2.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.2.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.2.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.2.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.2.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.2.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.2.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.2.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.2.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.2.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.2.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.2.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.2.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.2.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.3.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.3.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.3.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.3.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.3.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.3.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.3.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.3.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.3.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.3.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.3.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.3.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.3.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.3.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.3.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.3.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.4.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.4.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.4.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.4.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.4.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.4.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.4.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.4.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.4.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.4.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.4.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.4.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.4.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.4.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.4.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.4.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.5.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.5.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.5.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.5.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.5.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.5.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.5.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.5.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.5.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.5.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.5.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.5.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.5.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.5.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.5.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.5.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.6.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.6.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.6.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.6.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.6.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.6.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.6.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.6.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.6.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.6.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.6.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.6.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.7.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.7.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.7.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.7.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.7.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.7.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.7.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.7.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.7.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.7.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.7.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.7.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.8.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.8.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.8.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.8.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.8.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.8.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.8.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.8.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.8.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.8.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.8.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.8.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.9.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.9.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.9.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.9.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.9.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.9.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.9.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.9.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.9.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.9.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.9.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.9.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.10.attention.self.query.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.self.query.bias         |        [768] |     768 |
| encoder.encoder.layer.10.attention.self.key.weight         |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.self.key.bias           |        [768] |     768 |
| encoder.encoder.layer.10.attention.self.value.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.self.value.bias         |        [768] |     768 |
| encoder.encoder.layer.10.attention.output.dense.weight     |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.output.dense.bias       |        [768] |     768 |
| encoder.encoder.layer.10.attention.output.LayerNorm.weight |        [768] |     768 |
| encoder.encoder.layer.10.attention.output.LayerNorm.bias   |        [768] |     768 |
| encoder.encoder.layer.10.intermediate.dense.weight         |  [3072, 768] | 2359296 |
| encoder.encoder.layer.10.intermediate.dense.bias           |       [3072] |    3072 |
| encoder.encoder.layer.10.output.dense.weight               |  [768, 3072] | 2359296 |
| encoder.encoder.layer.10.output.dense.bias                 |        [768] |     768 |
| encoder.encoder.layer.10.output.LayerNorm.weight           |        [768] |     768 |
| encoder.encoder.layer.10.output.LayerNorm.bias             |        [768] |     768 |
| encoder.encoder.layer.11.attention.self.query.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.self.query.bias         |        [768] |     768 |
| encoder.encoder.layer.11.attention.self.key.weight         |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.self.key.bias           |        [768] |     768 |
| encoder.encoder.layer.11.attention.self.value.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.self.value.bias         |        [768] |     768 |
| encoder.encoder.layer.11.attention.output.dense.weight     |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.output.dense.bias       |        [768] |     768 |
| encoder.encoder.layer.11.attention.output.LayerNorm.weight |        [768] |     768 |
| encoder.encoder.layer.11.attention.output.LayerNorm.bias   |        [768] |     768 |
| encoder.encoder.layer.11.intermediate.dense.weight         |  [3072, 768] | 2359296 |
| encoder.encoder.layer.11.intermediate.dense.bias           |       [3072] |    3072 |
| encoder.encoder.layer.11.output.dense.weight               |  [768, 3072] | 2359296 |
| encoder.encoder.layer.11.output.dense.bias                 |        [768] |     768 |
| encoder.encoder.layer.11.output.LayerNorm.weight           |        [768] |     768 |
| encoder.encoder.layer.11.output.LayerNorm.bias             |        [768] |     768 |
| encoder.pooler.dense.weight                                |   [768, 768] |  589824 |
| encoder.pooler.dense.bias                                  |        [768] |     768 |
+------------------------------------------------------------+--------------+---------+
10/09/2022 07:08:39 - INFO - __main__ -   *** Example ***
10/09/2022 07:08:39 - INFO - __main__ -   idx: 0
10/09/2022 07:08:39 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_split', '_', 'phy', 'log', 'en', 'y', '_(', '_p', '_,', '_level', '_=', '_"', 's', '"', '_)', '_:', '_level', '_=', '_level', '_+', '_"__', '"', '_result', '_=', '_p', '_.', '_split', '_(', '_level', '_)', '_return', '_result', '_[', '_0', '_]', '_+', '_level', '_+', '_result', '_[', '_1', '_]', '_.', '_split', '_(', '_";"', '_)', '_[', '_0', '_]', '</s>']
10/09/2022 07:08:39 - INFO - __main__ -   code_ids: 0 6 2 729 5192 181 3258 896 386 207 400 428 2019 3144 385 437 201 120 743 545 3144 385 3144 513 12945 120 1046 385 428 746 5192 400 3144 743 483 1046 626 461 2406 513 3144 513 1046 626 524 2406 746 5192 400 29760 743 626 461 2406 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:08:39 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Return', '_either', '_the', '_full', '_or', '_truncated', '_version', '_of', '_a', '_Q', 'II', 'ME', '_-', '_formatted', '_taxonomy', '_string', '_.', '</s>']
10/09/2022 07:08:39 - INFO - __main__ -   nl_ids: 0 6 2 1675 4759 448 3662 872 19307 2229 595 434 1152 4300 1098 581 10440 29021 724 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:08:39 - INFO - __main__ -   *** Example ***
10/09/2022 07:08:39 - INFO - __main__ -   idx: 1
10/09/2022 07:08:39 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_ensure', '_', 'dir', '_(', '_d', '_)', '_:', '_if', '_not', '_os', '_.', '_path', '_.', '_exists', '_(', '_d', '_)', '_:', '_try', '_:', '_os', '_.', '_m', 'akedirs', '_(', '_d', '_)', '_except', '_OSError', '_as', '_oe', '_:', '_#', '_should', '_not', '_happen', '_with', '_os', '.', 'makedirs', '_#', '_ENOENT', ':', '_No', '_such', '_file', '_or', '_directory', '_if', '_os', '_.', '_errno', '_==', '_errno', '_.', '_ENOENT', '_:', '_msg', '_=', '_tw', 'dd', '_(', '_"""', 'One', '_or', '_more', '_directories', '_in', '_the', '_path', '_({})', '_do', '_not', '_exist', '.', '_If', 'Ċ', '__________________________', '_you', '_are', '_specifying', '_a', '_new', '_directory', '_for', '_output', ',', '_please', '_ensure', 'Ċ', '__________________________', '_all', '_other', '_directories', '_in', '_the', '_path', '_currently', '_exist', '."""', '_)', '_return', '_msg', '_.', '_format', '_(', '_d', '_)', '_else', '_:', '_msg', '_=', '_tw', 'dd', '_(', '_"""', 'An', '_error', '_occurred', '_trying', '_to', '_create', '_the', '_output', '_directory', 'Ċ', '__________________________', '_({})', '_with', '_message', ':', '_{}', '"""', '_)', '_return', '_msg', '_.', '_format', '_(', '_d', '_,', '_oe', '_.', '_strerror', '_)', '</s>']
10/09/2022 07:08:39 - INFO - __main__ -   code_ids: 0 6 2 729 6229 181 1282 400 480 743 545 462 800 2215 746 1391 746 4534 400 480 743 545 1568 545 2215 746 446 23328 400 480 743 3552 22934 880 44902 545 830 1570 800 7564 918 2215 132 24429 830 41059 144 4038 5632 1012 872 3456 462 2215 746 2341 550 2341 746 41059 545 2345 385 7916 443 400 1638 3533 872 2726 11613 488 448 1391 46072 1000 800 3040 132 1359 317 4584 2713 1147 15323 434 579 3456 563 1721 130 13874 6229 317 4584 1345 1946 11613 488 448 1391 6418 3040 6315 743 483 2345 746 2021 400 480 743 669 545 2345 385 7916 443 400 1638 1088 843 10058 11749 508 1738 448 1721 3456 317 4584 46072 918 1841 144 2334 3947 743 483 2345 746 2021 400 480 2019 44902 746 20115 743 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:08:39 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Check', '_to', '_make', '_sure', '_the', '_supplied', '_directory', '_path', '_does', '_not', '_exist', '_if', '_so', '_create', '_it', '_.', '_The', '_method', '_catch', 'es', '_OSError', '_exceptions', '_and', '_returns', '_a', '_desc', 'riptive', '_message', '_instead', '_of', '_re', '_-', '_raising', '_the', '_error', '_.', '</s>']
10/09/2022 07:08:39 - INFO - __main__ -   nl_ids: 0 6 2 1749 508 2002 3984 448 8813 3456 1391 2129 800 3040 462 1769 1738 835 746 1044 1454 2092 482 22934 12300 706 2060 434 2162 44105 1841 4488 595 479 581 47183 448 843 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:08:39 - INFO - __main__ -   *** Example ***
10/09/2022 07:08:39 - INFO - __main__ -   idx: 2
10/09/2022 07:08:39 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_file', '_', 'handle', '_(', '_fn', 'h', '_,', '_mode', '_=', '_"', 'r', 'U', '"', '_)', '_:', '_handle', '_=', '_None', '_if', '_isinstance', '_(', '_fn', 'h', '_,', '_file', '_)', '_:', '_if', '_fn', 'h', '_.', '_closed', '_:', '_raise', '_ValueError', '_(', '_"', 'Input', '_file', '_is', '_closed', '."', '_)', '_handle', '_=', '_fn', 'h', '_elif', '_isinstance', '_(', '_fn', 'h', '_,', '_str', '_)', '_:', '_handle', '_=', '_open', '_(', '_fn', 'h', '_,', '_mode', '_)', '_return', '_handle', '</s>']
10/09/2022 07:08:39 - INFO - __main__ -   code_ids: 0 6 2 729 1012 181 2133 400 4065 190 2019 2119 385 437 200 171 120 743 545 2384 385 1938 462 5408 400 4065 190 2019 1012 743 545 462 4065 190 746 8264 545 3085 6052 400 437 1834 1012 555 8264 3508 743 2384 385 4065 190 3625 5408 400 4065 190 2019 1113 743 545 2384 385 2717 400 4065 190 2019 2119 743 483 2384 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:08:39 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Takes', '_either', '_a', '_file', '_path', '_or', '_an', '_open', '_file', '_handle', '_checks', '_validity', '_and', '_returns', '_an', '_open', '_file', '_handle', '_or', '_raises', '_an', '_appropriate', '_Exception', '_.', '</s>']
10/09/2022 07:08:39 - INFO - __main__ -   nl_ids: 0 6 2 27408 4759 434 1012 1391 872 817 2717 1012 2384 7825 25911 706 2060 817 2717 1012 2384 872 23154 817 7900 2654 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:08:40 - INFO - __main__ -   ***** Running training *****
10/09/2022 07:08:40 - INFO - __main__ -     Num examples = 251820
10/09/2022 07:08:40 - INFO - __main__ -     Num Epochs = 10
10/09/2022 07:08:40 - INFO - __main__ -     Instantaneous batch size per GPU = 128
10/09/2022 07:08:40 - INFO - __main__ -     Total train batch size  = 128
10/09/2022 07:08:40 - INFO - __main__ -     Total optimization steps = 19680
10/09/2022 07:09:34 - INFO - __main__ -   epoch 0 step 100 loss 0.20605
10/09/2022 07:10:27 - INFO - __main__ -   epoch 0 step 200 loss 0.1507
10/09/2022 07:11:20 - INFO - __main__ -   epoch 0 step 300 loss 0.13938
10/09/2022 07:12:12 - INFO - __main__ -   epoch 0 step 400 loss 0.13121
10/09/2022 07:13:05 - INFO - __main__ -   epoch 0 step 500 loss 0.13702
10/09/2022 07:13:57 - INFO - __main__ -   epoch 0 step 600 loss 0.13097
10/09/2022 07:14:50 - INFO - __main__ -   epoch 0 step 700 loss 0.12817
10/09/2022 07:15:43 - INFO - __main__ -   epoch 0 step 800 loss 0.11928
10/09/2022 07:16:35 - INFO - __main__ -   epoch 0 step 900 loss 0.11732
10/09/2022 07:17:28 - INFO - __main__ -   epoch 0 step 1000 loss 0.11052
10/09/2022 07:18:21 - INFO - __main__ -   epoch 0 step 1100 loss 0.10622
10/09/2022 07:19:14 - INFO - __main__ -   epoch 0 step 1200 loss 0.11859
10/09/2022 07:20:06 - INFO - __main__ -   epoch 0 step 1300 loss 0.10106
10/09/2022 07:20:59 - INFO - __main__ -   epoch 0 step 1400 loss 0.11514
10/09/2022 07:21:52 - INFO - __main__ -   epoch 0 step 1500 loss 0.11512
10/09/2022 07:22:45 - INFO - __main__ -   epoch 0 step 1600 loss 0.10808
10/09/2022 07:23:37 - INFO - __main__ -   epoch 0 step 1700 loss 0.10894
10/09/2022 07:24:30 - INFO - __main__ -   epoch 0 step 1800 loss 0.1046
10/09/2022 07:25:23 - INFO - __main__ -   epoch 0 step 1900 loss 0.11357
10/09/2022 07:26:38 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:26:38 - INFO - __main__ -     Num queries = 13914
10/09/2022 07:26:38 - INFO - __main__ -     Num codes = 43827
10/09/2022 07:26:38 - INFO - __main__ -     Batch size = 128
10/09/2022 07:28:30 - INFO - __main__ -     R@1 = 0.61
10/09/2022 07:28:30 - INFO - __main__ -     R@5 = 0.832
10/09/2022 07:28:30 - INFO - __main__ -     R@10 = 0.881
10/09/2022 07:28:30 - INFO - __main__ -     eval_mrr = 0.71
10/09/2022 07:28:30 - INFO - __main__ -     ********************
10/09/2022 07:28:30 - INFO - __main__ -     Best mrr:0.71
10/09/2022 07:28:30 - INFO - __main__ -     ********************
10/09/2022 07:28:41 - INFO - __main__ -   Saving model checkpoint to saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_0_layers/20221009070454/checkpoint-best-mrr/model.bin
10/09/2022 07:29:34 - INFO - __main__ -   epoch 1 step 100 loss 0.08451
10/09/2022 07:30:27 - INFO - __main__ -   epoch 1 step 200 loss 0.07488
10/09/2022 07:31:20 - INFO - __main__ -   epoch 1 step 300 loss 0.07059
10/09/2022 07:32:12 - INFO - __main__ -   epoch 1 step 400 loss 0.07115
10/09/2022 07:33:05 - INFO - __main__ -   epoch 1 step 500 loss 0.07465
10/09/2022 07:33:58 - INFO - __main__ -   epoch 1 step 600 loss 0.07512
10/09/2022 07:34:50 - INFO - __main__ -   epoch 1 step 700 loss 0.06946
10/09/2022 07:35:43 - INFO - __main__ -   epoch 1 step 800 loss 0.07843
10/09/2022 07:36:36 - INFO - __main__ -   epoch 1 step 900 loss 0.07728
10/09/2022 07:37:29 - INFO - __main__ -   epoch 1 step 1000 loss 0.07651
10/09/2022 07:38:21 - INFO - __main__ -   epoch 1 step 1100 loss 0.07523
10/09/2022 07:39:14 - INFO - __main__ -   epoch 1 step 1200 loss 0.07176
10/09/2022 07:40:07 - INFO - __main__ -   epoch 1 step 1300 loss 0.07268
10/09/2022 07:40:59 - INFO - __main__ -   epoch 1 step 1400 loss 0.07188
10/09/2022 07:41:52 - INFO - __main__ -   epoch 1 step 1500 loss 0.06944
10/09/2022 07:42:45 - INFO - __main__ -   epoch 1 step 1600 loss 0.07143
10/09/2022 07:43:38 - INFO - __main__ -   epoch 1 step 1700 loss 0.07385
10/09/2022 07:44:30 - INFO - __main__ -   epoch 1 step 1800 loss 0.07417
10/09/2022 07:45:23 - INFO - __main__ -   epoch 1 step 1900 loss 0.07182
10/09/2022 07:46:32 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:46:32 - INFO - __main__ -     Num queries = 13914
10/09/2022 07:46:32 - INFO - __main__ -     Num codes = 43827
10/09/2022 07:46:32 - INFO - __main__ -     Batch size = 128
10/09/2022 07:48:22 - INFO - __main__ -     R@1 = 0.617
10/09/2022 07:48:22 - INFO - __main__ -     R@5 = 0.834
10/09/2022 07:48:22 - INFO - __main__ -     R@10 = 0.884
10/09/2022 07:48:22 - INFO - __main__ -     eval_mrr = 0.714
10/09/2022 07:48:22 - INFO - __main__ -     ********************
10/09/2022 07:48:22 - INFO - __main__ -     Best mrr:0.714
10/09/2022 07:48:22 - INFO - __main__ -     ********************
10/09/2022 07:48:30 - INFO - __main__ -   Saving model checkpoint to saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_0_layers/20221009070454/checkpoint-best-mrr/model.bin
10/09/2022 07:49:23 - INFO - __main__ -   epoch 2 step 100 loss 0.05711
10/09/2022 07:50:16 - INFO - __main__ -   epoch 2 step 200 loss 0.04621
10/09/2022 07:51:09 - INFO - __main__ -   epoch 2 step 300 loss 0.04729
10/09/2022 07:52:01 - INFO - __main__ -   epoch 2 step 400 loss 0.04781
10/09/2022 07:52:54 - INFO - __main__ -   epoch 2 step 500 loss 0.05055
10/09/2022 07:53:47 - INFO - __main__ -   epoch 2 step 600 loss 0.05108
10/09/2022 07:54:39 - INFO - __main__ -   epoch 2 step 700 loss 0.04929
10/09/2022 07:55:32 - INFO - __main__ -   epoch 2 step 800 loss 0.04678
10/09/2022 07:56:25 - INFO - __main__ -   epoch 2 step 900 loss 0.05348
10/09/2022 07:57:18 - INFO - __main__ -   epoch 2 step 1000 loss 0.05048
10/09/2022 07:58:10 - INFO - __main__ -   epoch 2 step 1100 loss 0.05103
10/09/2022 07:59:03 - INFO - __main__ -   epoch 2 step 1200 loss 0.05171
10/09/2022 07:59:56 - INFO - __main__ -   epoch 2 step 1300 loss 0.05363
10/09/2022 08:00:48 - INFO - __main__ -   epoch 2 step 1400 loss 0.05404
10/09/2022 08:01:41 - INFO - __main__ -   epoch 2 step 1500 loss 0.04929
10/09/2022 08:02:34 - INFO - __main__ -   epoch 2 step 1600 loss 0.05258
10/09/2022 08:03:27 - INFO - __main__ -   epoch 2 step 1700 loss 0.0508
10/09/2022 08:04:19 - INFO - __main__ -   epoch 2 step 1800 loss 0.05149
10/09/2022 08:05:12 - INFO - __main__ -   epoch 2 step 1900 loss 0.0525
10/09/2022 08:06:21 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 08:06:21 - INFO - __main__ -     Num queries = 13914
10/09/2022 08:06:21 - INFO - __main__ -     Num codes = 43827
10/09/2022 08:06:21 - INFO - __main__ -     Batch size = 128
10/09/2022 08:08:12 - INFO - __main__ -     R@1 = 0.618
10/09/2022 08:08:12 - INFO - __main__ -     R@5 = 0.835
10/09/2022 08:08:12 - INFO - __main__ -     R@10 = 0.884
10/09/2022 08:08:12 - INFO - __main__ -     eval_mrr = 0.715
10/09/2022 08:08:12 - INFO - __main__ -     ********************
10/09/2022 08:08:12 - INFO - __main__ -     Best mrr:0.715
10/09/2022 08:08:12 - INFO - __main__ -     ********************
10/09/2022 08:08:21 - INFO - __main__ -   Saving model checkpoint to saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_0_layers/20221009070454/checkpoint-best-mrr/model.bin
10/09/2022 08:09:14 - INFO - __main__ -   epoch 3 step 100 loss 0.04298
10/09/2022 08:10:07 - INFO - __main__ -   epoch 3 step 200 loss 0.03528
10/09/2022 08:11:00 - INFO - __main__ -   epoch 3 step 300 loss 0.03847
10/09/2022 08:11:52 - INFO - __main__ -   epoch 3 step 400 loss 0.03728
10/09/2022 08:12:45 - INFO - __main__ -   epoch 3 step 500 loss 0.03923
10/09/2022 08:13:38 - INFO - __main__ -   epoch 3 step 600 loss 0.03633
10/09/2022 08:14:30 - INFO - __main__ -   epoch 3 step 700 loss 0.03825
10/09/2022 08:15:23 - INFO - __main__ -   epoch 3 step 800 loss 0.03707
10/09/2022 08:16:16 - INFO - __main__ -   epoch 3 step 900 loss 0.03838
10/09/2022 08:17:08 - INFO - __main__ -   epoch 3 step 1000 loss 0.03904
10/09/2022 08:18:01 - INFO - __main__ -   epoch 3 step 1100 loss 0.03634
10/09/2022 08:18:54 - INFO - __main__ -   epoch 3 step 1200 loss 0.03773
10/09/2022 08:19:46 - INFO - __main__ -   epoch 3 step 1300 loss 0.03888
10/09/2022 08:20:39 - INFO - __main__ -   epoch 3 step 1400 loss 0.03808
10/09/2022 08:21:32 - INFO - __main__ -   epoch 3 step 1500 loss 0.03346
10/09/2022 08:22:24 - INFO - __main__ -   epoch 3 step 1600 loss 0.03776
10/09/2022 08:23:17 - INFO - __main__ -   epoch 3 step 1700 loss 0.03642
10/09/2022 08:24:10 - INFO - __main__ -   epoch 3 step 1800 loss 0.03687
10/09/2022 08:25:03 - INFO - __main__ -   epoch 3 step 1900 loss 0.0383
10/09/2022 08:26:12 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 08:26:12 - INFO - __main__ -     Num queries = 13914
10/09/2022 08:26:12 - INFO - __main__ -     Num codes = 43827
10/09/2022 08:26:12 - INFO - __main__ -     Batch size = 128
10/09/2022 08:28:03 - INFO - __main__ -     R@1 = 0.61
10/09/2022 08:28:03 - INFO - __main__ -     R@5 = 0.831
10/09/2022 08:28:03 - INFO - __main__ -     R@10 = 0.884
10/09/2022 08:28:03 - INFO - __main__ -     eval_mrr = 0.709
10/09/2022 08:28:56 - INFO - __main__ -   epoch 4 step 100 loss 0.03113
10/09/2022 08:29:49 - INFO - __main__ -   epoch 4 step 200 loss 0.02788
10/09/2022 08:30:41 - INFO - __main__ -   epoch 4 step 300 loss 0.0314
10/09/2022 08:31:34 - INFO - __main__ -   epoch 4 step 400 loss 0.02977
10/09/2022 08:32:27 - INFO - __main__ -   epoch 4 step 500 loss 0.03013
10/09/2022 08:33:19 - INFO - __main__ -   epoch 4 step 600 loss 0.02992
10/09/2022 08:34:12 - INFO - __main__ -   epoch 4 step 700 loss 0.02929
10/09/2022 08:35:05 - INFO - __main__ -   epoch 4 step 800 loss 0.02883
10/09/2022 08:35:58 - INFO - __main__ -   epoch 4 step 900 loss 0.02955
10/09/2022 08:36:50 - INFO - __main__ -   epoch 4 step 1000 loss 0.02966
10/09/2022 08:37:43 - INFO - __main__ -   epoch 4 step 1100 loss 0.0283
10/09/2022 08:38:36 - INFO - __main__ -   epoch 4 step 1200 loss 0.03011
10/09/2022 08:39:28 - INFO - __main__ -   epoch 4 step 1300 loss 0.03067
10/09/2022 08:40:21 - INFO - __main__ -   epoch 4 step 1400 loss 0.02876
10/09/2022 08:41:14 - INFO - __main__ -   epoch 4 step 1500 loss 0.03093
10/09/2022 08:42:06 - INFO - __main__ -   epoch 4 step 1600 loss 0.03049
10/09/2022 08:42:59 - INFO - __main__ -   epoch 4 step 1700 loss 0.02873
10/09/2022 08:43:52 - INFO - __main__ -   epoch 4 step 1800 loss 0.03093
10/09/2022 08:44:45 - INFO - __main__ -   epoch 4 step 1900 loss 0.02986
10/09/2022 08:45:53 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 08:45:53 - INFO - __main__ -     Num queries = 13914
10/09/2022 08:45:53 - INFO - __main__ -     Num codes = 43827
10/09/2022 08:45:53 - INFO - __main__ -     Batch size = 128
10/09/2022 08:47:44 - INFO - __main__ -     R@1 = 0.614
10/09/2022 08:47:44 - INFO - __main__ -     R@5 = 0.834
10/09/2022 08:47:44 - INFO - __main__ -     R@10 = 0.884
10/09/2022 08:47:44 - INFO - __main__ -     eval_mrr = 0.712
10/09/2022 08:48:38 - INFO - __main__ -   epoch 5 step 100 loss 0.02561
10/09/2022 08:49:30 - INFO - __main__ -   epoch 5 step 200 loss 0.02493
10/09/2022 08:50:23 - INFO - __main__ -   epoch 5 step 300 loss 0.02423
10/09/2022 08:51:16 - INFO - __main__ -   epoch 5 step 400 loss 0.02372
10/09/2022 08:52:08 - INFO - __main__ -   epoch 5 step 500 loss 0.0247
10/09/2022 08:53:01 - INFO - __main__ -   epoch 5 step 600 loss 0.02411
10/09/2022 08:53:54 - INFO - __main__ -   epoch 5 step 700 loss 0.02401
10/09/2022 08:54:47 - INFO - __main__ -   epoch 5 step 800 loss 0.02534
10/09/2022 08:55:39 - INFO - __main__ -   epoch 5 step 900 loss 0.02586
10/09/2022 08:56:32 - INFO - __main__ -   epoch 5 step 1000 loss 0.02412
10/09/2022 08:57:25 - INFO - __main__ -   epoch 5 step 1100 loss 0.02449
10/09/2022 08:58:18 - INFO - __main__ -   epoch 5 step 1200 loss 0.02308
10/09/2022 08:59:10 - INFO - __main__ -   epoch 5 step 1300 loss 0.02499
10/09/2022 09:00:03 - INFO - __main__ -   epoch 5 step 1400 loss 0.02428
10/09/2022 09:00:56 - INFO - __main__ -   epoch 5 step 1500 loss 0.026
10/09/2022 09:01:49 - INFO - __main__ -   epoch 5 step 1600 loss 0.02559
10/09/2022 09:02:41 - INFO - __main__ -   epoch 5 step 1700 loss 0.0242
10/09/2022 09:03:34 - INFO - __main__ -   epoch 5 step 1800 loss 0.02719
10/09/2022 09:04:27 - INFO - __main__ -   epoch 5 step 1900 loss 0.02647
10/09/2022 09:05:36 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 09:05:36 - INFO - __main__ -     Num queries = 13914
10/09/2022 09:05:36 - INFO - __main__ -     Num codes = 43827
10/09/2022 09:05:36 - INFO - __main__ -     Batch size = 128
10/09/2022 09:07:27 - INFO - __main__ -     R@1 = 0.617
10/09/2022 09:07:27 - INFO - __main__ -     R@5 = 0.834
10/09/2022 09:07:27 - INFO - __main__ -     R@10 = 0.885
10/09/2022 09:07:27 - INFO - __main__ -     eval_mrr = 0.715
10/09/2022 09:08:20 - INFO - __main__ -   epoch 6 step 100 loss 0.02267
10/09/2022 09:09:13 - INFO - __main__ -   epoch 6 step 200 loss 0.02318
10/09/2022 09:10:05 - INFO - __main__ -   epoch 6 step 300 loss 0.02106
10/09/2022 09:10:58 - INFO - __main__ -   epoch 6 step 400 loss 0.02127
10/09/2022 09:11:51 - INFO - __main__ -   epoch 6 step 500 loss 0.02153
10/09/2022 09:12:44 - INFO - __main__ -   epoch 6 step 600 loss 0.02189
10/09/2022 09:13:36 - INFO - __main__ -   epoch 6 step 700 loss 0.01991
10/09/2022 09:14:29 - INFO - __main__ -   epoch 6 step 800 loss 0.02107
10/09/2022 09:15:22 - INFO - __main__ -   epoch 6 step 900 loss 0.02123
10/09/2022 09:16:15 - INFO - __main__ -   epoch 6 step 1000 loss 0.02128
10/09/2022 09:17:07 - INFO - __main__ -   epoch 6 step 1100 loss 0.02142
10/09/2022 09:18:00 - INFO - __main__ -   epoch 6 step 1200 loss 0.02057
10/09/2022 09:18:53 - INFO - __main__ -   epoch 6 step 1300 loss 0.02039
10/09/2022 09:19:46 - INFO - __main__ -   epoch 6 step 1400 loss 0.02329
10/09/2022 09:20:38 - INFO - __main__ -   epoch 6 step 1500 loss 0.02317
10/09/2022 09:21:31 - INFO - __main__ -   epoch 6 step 1600 loss 0.02174
10/09/2022 09:22:24 - INFO - __main__ -   epoch 6 step 1700 loss 0.02221
10/09/2022 09:23:17 - INFO - __main__ -   epoch 6 step 1800 loss 0.02192
10/09/2022 09:24:09 - INFO - __main__ -   epoch 6 step 1900 loss 0.02225
10/09/2022 09:25:18 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 09:25:18 - INFO - __main__ -     Num queries = 13914
10/09/2022 09:25:18 - INFO - __main__ -     Num codes = 43827
10/09/2022 09:25:18 - INFO - __main__ -     Batch size = 128
10/09/2022 09:27:10 - INFO - __main__ -     R@1 = 0.619
10/09/2022 09:27:10 - INFO - __main__ -     R@5 = 0.835
10/09/2022 09:27:10 - INFO - __main__ -     R@10 = 0.885
10/09/2022 09:27:10 - INFO - __main__ -     eval_mrr = 0.716
10/09/2022 09:27:10 - INFO - __main__ -     ********************
10/09/2022 09:27:10 - INFO - __main__ -     Best mrr:0.716
10/09/2022 09:27:10 - INFO - __main__ -     ********************
10/09/2022 09:27:16 - INFO - __main__ -   Saving model checkpoint to saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_0_layers/20221009070454/checkpoint-best-mrr/model.bin
10/09/2022 09:28:09 - INFO - __main__ -   epoch 7 step 100 loss 0.02007
10/09/2022 09:29:02 - INFO - __main__ -   epoch 7 step 200 loss 0.01988
10/09/2022 09:29:55 - INFO - __main__ -   epoch 7 step 300 loss 0.01901
10/09/2022 09:30:47 - INFO - __main__ -   epoch 7 step 400 loss 0.01916
10/09/2022 09:31:40 - INFO - __main__ -   epoch 7 step 500 loss 0.01933
10/09/2022 09:32:33 - INFO - __main__ -   epoch 7 step 600 loss 0.02158
10/09/2022 09:33:26 - INFO - __main__ -   epoch 7 step 700 loss 0.0198
10/09/2022 09:34:19 - INFO - __main__ -   epoch 7 step 800 loss 0.01826
10/09/2022 09:35:11 - INFO - __main__ -   epoch 7 step 900 loss 0.01853
10/09/2022 09:36:04 - INFO - __main__ -   epoch 7 step 1000 loss 0.0189
10/09/2022 09:36:57 - INFO - __main__ -   epoch 7 step 1100 loss 0.02048
10/09/2022 09:37:49 - INFO - __main__ -   epoch 7 step 1200 loss 0.01846
10/09/2022 09:38:42 - INFO - __main__ -   epoch 7 step 1300 loss 0.02025
10/09/2022 09:39:35 - INFO - __main__ -   epoch 7 step 1400 loss 0.02085
10/09/2022 09:40:28 - INFO - __main__ -   epoch 7 step 1500 loss 0.01866
10/09/2022 09:41:20 - INFO - __main__ -   epoch 7 step 1600 loss 0.01959
10/09/2022 09:42:13 - INFO - __main__ -   epoch 7 step 1700 loss 0.01833
10/09/2022 09:43:06 - INFO - __main__ -   epoch 7 step 1800 loss 0.01988
10/09/2022 09:43:58 - INFO - __main__ -   epoch 7 step 1900 loss 0.01938
10/09/2022 09:45:05 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 09:45:05 - INFO - __main__ -     Num queries = 13914
10/09/2022 09:45:05 - INFO - __main__ -     Num codes = 43827
10/09/2022 09:45:05 - INFO - __main__ -     Batch size = 128
10/09/2022 09:46:56 - INFO - __main__ -     R@1 = 0.619
10/09/2022 09:46:56 - INFO - __main__ -     R@5 = 0.837
10/09/2022 09:46:56 - INFO - __main__ -     R@10 = 0.886
10/09/2022 09:46:56 - INFO - __main__ -     eval_mrr = 0.716
10/09/2022 09:47:51 - INFO - __main__ -   epoch 8 step 100 loss 0.01864
10/09/2022 09:48:44 - INFO - __main__ -   epoch 8 step 200 loss 0.01733
10/09/2022 09:49:37 - INFO - __main__ -   epoch 8 step 300 loss 0.01888
10/09/2022 09:50:30 - INFO - __main__ -   epoch 8 step 400 loss 0.0173
10/09/2022 09:51:22 - INFO - __main__ -   epoch 8 step 500 loss 0.01796
10/09/2022 09:52:15 - INFO - __main__ -   epoch 8 step 600 loss 0.01845
10/09/2022 09:53:08 - INFO - __main__ -   epoch 8 step 700 loss 0.0172
10/09/2022 09:54:00 - INFO - __main__ -   epoch 8 step 800 loss 0.0183
10/09/2022 09:54:53 - INFO - __main__ -   epoch 8 step 900 loss 0.01925
10/09/2022 09:55:46 - INFO - __main__ -   epoch 8 step 1000 loss 0.01849
10/09/2022 09:56:39 - INFO - __main__ -   epoch 8 step 1100 loss 0.01787
10/09/2022 09:57:31 - INFO - __main__ -   epoch 8 step 1200 loss 0.01985
10/09/2022 09:58:24 - INFO - __main__ -   epoch 8 step 1300 loss 0.01872
10/09/2022 09:59:17 - INFO - __main__ -   epoch 8 step 1400 loss 0.01756
10/09/2022 10:00:10 - INFO - __main__ -   epoch 8 step 1500 loss 0.01917
10/09/2022 10:01:02 - INFO - __main__ -   epoch 8 step 1600 loss 0.01697
10/09/2022 10:01:55 - INFO - __main__ -   epoch 8 step 1700 loss 0.01815
10/09/2022 10:02:48 - INFO - __main__ -   epoch 8 step 1800 loss 0.01846
10/09/2022 10:03:41 - INFO - __main__ -   epoch 8 step 1900 loss 0.01825
10/09/2022 10:04:49 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 10:04:49 - INFO - __main__ -     Num queries = 13914
10/09/2022 10:04:49 - INFO - __main__ -     Num codes = 43827
10/09/2022 10:04:49 - INFO - __main__ -     Batch size = 128
10/09/2022 10:06:40 - INFO - __main__ -     R@1 = 0.622
10/09/2022 10:06:40 - INFO - __main__ -     R@5 = 0.837
10/09/2022 10:06:40 - INFO - __main__ -     R@10 = 0.886
10/09/2022 10:06:40 - INFO - __main__ -     eval_mrr = 0.718
10/09/2022 10:06:40 - INFO - __main__ -     ********************
10/09/2022 10:06:40 - INFO - __main__ -     Best mrr:0.718
10/09/2022 10:06:40 - INFO - __main__ -     ********************
10/09/2022 10:06:48 - INFO - __main__ -   Saving model checkpoint to saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_0_layers/20221009070454/checkpoint-best-mrr/model.bin
10/09/2022 10:07:41 - INFO - __main__ -   epoch 9 step 100 loss 0.01915
10/09/2022 10:08:34 - INFO - __main__ -   epoch 9 step 200 loss 0.0178
10/09/2022 10:09:26 - INFO - __main__ -   epoch 9 step 300 loss 0.01891
10/09/2022 10:10:19 - INFO - __main__ -   epoch 9 step 400 loss 0.0177
10/09/2022 10:11:12 - INFO - __main__ -   epoch 9 step 500 loss 0.01832
10/09/2022 10:12:05 - INFO - __main__ -   epoch 9 step 600 loss 0.01649
10/09/2022 10:12:57 - INFO - __main__ -   epoch 9 step 700 loss 0.01594
10/09/2022 10:13:50 - INFO - __main__ -   epoch 9 step 800 loss 0.01748
10/09/2022 10:14:43 - INFO - __main__ -   epoch 9 step 900 loss 0.01748
10/09/2022 10:15:36 - INFO - __main__ -   epoch 9 step 1000 loss 0.01766
10/09/2022 10:16:28 - INFO - __main__ -   epoch 9 step 1100 loss 0.01667
10/09/2022 10:17:21 - INFO - __main__ -   epoch 9 step 1200 loss 0.01692
10/09/2022 10:18:14 - INFO - __main__ -   epoch 9 step 1300 loss 0.01722
10/09/2022 10:19:06 - INFO - __main__ -   epoch 9 step 1400 loss 0.01726
10/09/2022 10:19:59 - INFO - __main__ -   epoch 9 step 1500 loss 0.01742
10/09/2022 10:20:52 - INFO - __main__ -   epoch 9 step 1600 loss 0.01844
10/09/2022 10:21:44 - INFO - __main__ -   epoch 9 step 1700 loss 0.01701
10/09/2022 10:22:37 - INFO - __main__ -   epoch 9 step 1800 loss 0.01763
10/09/2022 10:23:30 - INFO - __main__ -   epoch 9 step 1900 loss 0.0163
10/09/2022 10:24:39 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 10:24:39 - INFO - __main__ -     Num queries = 13914
10/09/2022 10:24:39 - INFO - __main__ -     Num codes = 43827
10/09/2022 10:24:39 - INFO - __main__ -     Batch size = 128
10/09/2022 10:26:30 - INFO - __main__ -     R@1 = 0.622
10/09/2022 10:26:30 - INFO - __main__ -     R@5 = 0.836
10/09/2022 10:26:30 - INFO - __main__ -     R@10 = 0.886
10/09/2022 10:26:30 - INFO - __main__ -     eval_mrr = 0.717
10/09/2022 10:27:04 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 10:27:04 - INFO - __main__ -     Num queries = 13914
10/09/2022 10:27:04 - INFO - __main__ -     Num codes = 43827
10/09/2022 10:27:04 - INFO - __main__ -     Batch size = 128
10/09/2022 10:28:55 - INFO - __main__ -   ***** Eval results *****
10/09/2022 10:28:55 - INFO - __main__ -     R@1 = 0.622
10/09/2022 10:28:55 - INFO - __main__ -     R@10 = 0.886
10/09/2022 10:28:55 - INFO - __main__ -     R@5 = 0.837
10/09/2022 10:28:55 - INFO - __main__ -     eval_mrr = 0.718
10/09/2022 10:29:29 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 10:29:29 - INFO - __main__ -     Num queries = 14918
10/09/2022 10:29:29 - INFO - __main__ -     Num codes = 43827
10/09/2022 10:29:29 - INFO - __main__ -     Batch size = 128
10/09/2022 10:31:25 - INFO - __main__ -   ***** Eval results *****
10/09/2022 10:31:25 - INFO - __main__ -     R@1 = 0.63
10/09/2022 10:31:25 - INFO - __main__ -     R@10 = 0.895
10/09/2022 10:31:25 - INFO - __main__ -     R@5 = 0.846
10/09/2022 10:31:25 - INFO - __main__ -     eval_mrr = 0.727
10/09/2022 10:31:25 - INFO - utils -   saved dataset in saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_0_layers/20221009070454/result.jsonl
