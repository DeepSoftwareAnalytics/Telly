10/09/2022 07:04:59 - INFO - __main__ -   device: cuda, n_gpu: 1
10/09/2022 07:05:01 - DEBUG - filelock -   Attempting to acquire lock 140547468795136 on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
10/09/2022 07:05:01 - DEBUG - filelock -   Lock 140547468795136 acquired on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
Downloading:   0%|          | 0.00/916k [00:00<?, ?B/s]Downloading:   4%|▍         | 40.0k/916k [00:00<00:02, 386kB/s]Downloading:  19%|█▉        | 172k/916k [00:00<00:00, 894kB/s] Downloading:  76%|███████▋  | 700k/916k [00:00<00:00, 2.76MB/s]Downloading: 100%|██████████| 916k/916k [00:00<00:00, 2.86MB/s]
10/09/2022 07:05:01 - DEBUG - filelock -   Attempting to release lock 140547468795136 on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
10/09/2022 07:05:01 - DEBUG - filelock -   Lock 140547468795136 released on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
10/09/2022 07:05:01 - DEBUG - filelock -   Attempting to acquire lock 140547468794992 on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
10/09/2022 07:05:01 - DEBUG - filelock -   Lock 140547468794992 acquired on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
Downloading:   0%|          | 0.00/434k [00:00<?, ?B/s]Downloading:   8%|▊         | 36.0k/434k [00:00<00:01, 342kB/s]Downloading:  37%|███▋      | 160k/434k [00:00<00:00, 829kB/s] Downloading: 100%|██████████| 434k/434k [00:00<00:00, 1.62MB/s]
10/09/2022 07:05:02 - DEBUG - filelock -   Attempting to release lock 140547468794992 on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
10/09/2022 07:05:02 - DEBUG - filelock -   Lock 140547468794992 released on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
10/09/2022 07:05:02 - DEBUG - filelock -   Attempting to acquire lock 140547468794992 on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
10/09/2022 07:05:02 - DEBUG - filelock -   Lock 140547468794992 acquired on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]Downloading: 100%|██████████| 772/772 [00:00<00:00, 394kB/s]
10/09/2022 07:05:03 - DEBUG - filelock -   Attempting to release lock 140547468794992 on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
10/09/2022 07:05:03 - DEBUG - filelock -   Lock 140547468794992 released on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
10/09/2022 07:05:03 - DEBUG - filelock -   Attempting to acquire lock 140547468653808 on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
10/09/2022 07:05:03 - DEBUG - filelock -   Lock 140547468653808 acquired on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
Downloading:   0%|          | 0.00/1.09k [00:00<?, ?B/s]Downloading: 100%|██████████| 1.09k/1.09k [00:00<00:00, 919kB/s]
10/09/2022 07:05:03 - DEBUG - filelock -   Attempting to release lock 140547468653808 on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
10/09/2022 07:05:03 - DEBUG - filelock -   Lock 140547468653808 released on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
10/09/2022 07:05:04 - DEBUG - filelock -   Attempting to acquire lock 140547470468432 on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
10/09/2022 07:05:04 - DEBUG - filelock -   Lock 140547470468432 acquired on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
Downloading:   0%|          | 0.00/691 [00:00<?, ?B/s]Downloading: 100%|██████████| 691/691 [00:00<00:00, 569kB/s]
10/09/2022 07:05:04 - DEBUG - filelock -   Attempting to release lock 140547470468432 on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
10/09/2022 07:05:04 - DEBUG - filelock -   Lock 140547470468432 released on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
10/09/2022 07:05:05 - DEBUG - filelock -   Attempting to acquire lock 140547233229888 on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
10/09/2022 07:05:05 - DEBUG - filelock -   Lock 140547233229888 acquired on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
Downloading:   0%|          | 0.00/480M [00:00<?, ?B/s]Downloading:   1%|          | 4.65M/480M [00:00<00:10, 48.7MB/s]Downloading:   2%|▏         | 9.67M/480M [00:00<00:09, 51.0MB/s]Downloading:   4%|▍         | 19.3M/480M [00:00<00:06, 73.6MB/s]Downloading:   6%|▌         | 29.1M/480M [00:00<00:05, 85.4MB/s]Downloading:   8%|▊         | 39.6M/480M [00:00<00:04, 94.0MB/s]Downloading:  10%|█         | 50.1M/480M [00:00<00:04, 99.8MB/s]Downloading:  13%|█▎        | 60.7M/480M [00:00<00:04, 103MB/s] Downloading:  15%|█▍        | 70.9M/480M [00:00<00:04, 105MB/s]Downloading:  17%|█▋        | 81.0M/480M [00:00<00:03, 105MB/s]Downloading:  19%|█▉        | 91.2M/480M [00:01<00:03, 106MB/s]Downloading:  21%|██        | 101M/480M [00:01<00:03, 106MB/s] Downloading:  23%|██▎       | 112M/480M [00:01<00:03, 106MB/s]Downloading:  25%|██▌       | 122M/480M [00:01<00:03, 107MB/s]Downloading:  28%|██▊       | 132M/480M [00:01<00:03, 107MB/s]Downloading:  30%|██▉       | 142M/480M [00:01<00:03, 107MB/s]Downloading:  32%|███▏      | 153M/480M [00:01<00:03, 107MB/s]Downloading:  34%|███▍      | 163M/480M [00:01<00:03, 107MB/s]Downloading:  36%|███▌      | 173M/480M [00:01<00:03, 106MB/s]Downloading:  38%|███▊      | 183M/480M [00:01<00:02, 106MB/s]Downloading:  40%|████      | 193M/480M [00:02<00:02, 106MB/s]Downloading:  42%|████▏     | 204M/480M [00:02<00:02, 106MB/s]Downloading:  44%|████▍     | 214M/480M [00:02<00:02, 105MB/s]Downloading:  47%|████▋     | 224M/480M [00:02<00:02, 105MB/s]Downloading:  49%|████▊     | 234M/480M [00:02<00:02, 105MB/s]Downloading:  51%|█████     | 244M/480M [00:02<00:02, 105MB/s]Downloading:  53%|█████▎    | 254M/480M [00:02<00:02, 105MB/s]Downloading:  55%|█████▍    | 264M/480M [00:02<00:02, 105MB/s]Downloading:  57%|█████▋    | 274M/480M [00:02<00:02, 105MB/s]Downloading:  59%|█████▉    | 284M/480M [00:02<00:01, 105MB/s]Downloading:  61%|██████    | 294M/480M [00:03<00:01, 105MB/s]Downloading:  63%|██████▎   | 304M/480M [00:03<00:01, 105MB/s]Downloading:  65%|██████▌   | 314M/480M [00:03<00:01, 105MB/s]Downloading:  67%|██████▋   | 324M/480M [00:03<00:01, 105MB/s]Downloading:  70%|██████▉   | 334M/480M [00:03<00:01, 105MB/s]Downloading:  72%|███████▏  | 344M/480M [00:03<00:01, 105MB/s]Downloading:  74%|███████▎  | 354M/480M [00:03<00:01, 105MB/s]Downloading:  76%|███████▌  | 364M/480M [00:03<00:01, 105MB/s]Downloading:  78%|███████▊  | 374M/480M [00:03<00:01, 105MB/s]Downloading:  80%|███████▉  | 384M/480M [00:03<00:00, 105MB/s]Downloading:  82%|████████▏ | 394M/480M [00:04<00:00, 104MB/s]Downloading:  84%|████████▍ | 404M/480M [00:04<00:00, 105MB/s]Downloading:  86%|████████▌ | 414M/480M [00:04<00:00, 104MB/s]Downloading:  88%|████████▊ | 424M/480M [00:04<00:00, 105MB/s]Downloading:  90%|█████████ | 434M/480M [00:04<00:00, 105MB/s]Downloading:  93%|█████████▎| 445M/480M [00:04<00:00, 105MB/s]Downloading:  95%|█████████▍| 455M/480M [00:04<00:00, 105MB/s]Downloading:  97%|█████████▋| 465M/480M [00:04<00:00, 105MB/s]Downloading:  99%|█████████▉| 475M/480M [00:04<00:00, 105MB/s]Downloading: 100%|██████████| 480M/480M [00:04<00:00, 103MB/s]
10/09/2022 07:05:10 - DEBUG - filelock -   Attempting to release lock 140547233229888 on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
10/09/2022 07:05:10 - DEBUG - filelock -   Lock 140547233229888 released on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
10/09/2022 07:05:11 - INFO - __main__ -   Training/evaluation parameters Namespace(code_length=256, codebase_file='dataset/CSN/python/codebase.jsonl', config_name='', debug=False, device=device(type='cuda'), do_F2_norm=False, do_eval=True, do_test=True, do_train=True, do_zero_shot=False, eval_batch_size=128, eval_data_file='dataset/CSN/python/valid.jsonl', freeze_bottom_k_layer_index=3, learning_rate=2e-05, max_grad_norm=1.0, model_name_or_path='microsoft/unixcoder-base', n_debug_samples=100, n_gpu=1, nl_length=128, num_train_epochs=10, output_dir='saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_3_layers/20221009070453', seed=123456, test_data_file='dataset/CSN/python/test.jsonl', tokenizer_name='', train_batch_size=128, train_data_file='dataset/CSN/python/train.jsonl', weight_decay=0.01)
10/09/2022 07:05:11 - INFO - __main__ -   +------------------------------------------------------------+--------------+---------+
| Layer Name                                                 | Output Shape | Param # |
+------------------------------------------------------------+--------------+---------+
| encoder.encoder.layer.3.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.3.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.3.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.3.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.3.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.3.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.3.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.3.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.3.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.3.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.3.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.3.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.3.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.3.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.3.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.3.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.4.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.4.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.4.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.4.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.4.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.4.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.4.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.4.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.4.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.4.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.4.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.4.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.4.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.4.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.4.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.4.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.5.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.5.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.5.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.5.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.5.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.5.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.5.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.5.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.5.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.5.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.5.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.5.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.5.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.5.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.5.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.5.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.6.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.6.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.6.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.6.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.6.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.6.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.6.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.6.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.6.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.6.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.6.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.6.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.7.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.7.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.7.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.7.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.7.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.7.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.7.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.7.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.7.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.7.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.7.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.7.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.8.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.8.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.8.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.8.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.8.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.8.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.8.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.8.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.8.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.8.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.8.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.8.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.9.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.9.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.9.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.9.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.9.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.9.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.9.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.9.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.9.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.9.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.9.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.9.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.10.attention.self.query.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.self.query.bias         |        [768] |     768 |
| encoder.encoder.layer.10.attention.self.key.weight         |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.self.key.bias           |        [768] |     768 |
| encoder.encoder.layer.10.attention.self.value.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.self.value.bias         |        [768] |     768 |
| encoder.encoder.layer.10.attention.output.dense.weight     |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.output.dense.bias       |        [768] |     768 |
| encoder.encoder.layer.10.attention.output.LayerNorm.weight |        [768] |     768 |
| encoder.encoder.layer.10.attention.output.LayerNorm.bias   |        [768] |     768 |
| encoder.encoder.layer.10.intermediate.dense.weight         |  [3072, 768] | 2359296 |
| encoder.encoder.layer.10.intermediate.dense.bias           |       [3072] |    3072 |
| encoder.encoder.layer.10.output.dense.weight               |  [768, 3072] | 2359296 |
| encoder.encoder.layer.10.output.dense.bias                 |        [768] |     768 |
| encoder.encoder.layer.10.output.LayerNorm.weight           |        [768] |     768 |
| encoder.encoder.layer.10.output.LayerNorm.bias             |        [768] |     768 |
| encoder.encoder.layer.11.attention.self.query.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.self.query.bias         |        [768] |     768 |
| encoder.encoder.layer.11.attention.self.key.weight         |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.self.key.bias           |        [768] |     768 |
| encoder.encoder.layer.11.attention.self.value.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.self.value.bias         |        [768] |     768 |
| encoder.encoder.layer.11.attention.output.dense.weight     |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.output.dense.bias       |        [768] |     768 |
| encoder.encoder.layer.11.attention.output.LayerNorm.weight |        [768] |     768 |
| encoder.encoder.layer.11.attention.output.LayerNorm.bias   |        [768] |     768 |
| encoder.encoder.layer.11.intermediate.dense.weight         |  [3072, 768] | 2359296 |
| encoder.encoder.layer.11.intermediate.dense.bias           |       [3072] |    3072 |
| encoder.encoder.layer.11.output.dense.weight               |  [768, 3072] | 2359296 |
| encoder.encoder.layer.11.output.dense.bias                 |        [768] |     768 |
| encoder.encoder.layer.11.output.LayerNorm.weight           |        [768] |     768 |
| encoder.encoder.layer.11.output.LayerNorm.bias             |        [768] |     768 |
| encoder.pooler.dense.weight                                |   [768, 768] |  589824 |
| encoder.pooler.dense.bias                                  |        [768] |     768 |
+------------------------------------------------------------+--------------+---------+
10/09/2022 07:08:46 - INFO - __main__ -   *** Example ***
10/09/2022 07:08:46 - INFO - __main__ -   idx: 0
10/09/2022 07:08:46 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_split', '_', 'phy', 'log', 'en', 'y', '_(', '_p', '_,', '_level', '_=', '_"', 's', '"', '_)', '_:', '_level', '_=', '_level', '_+', '_"__', '"', '_result', '_=', '_p', '_.', '_split', '_(', '_level', '_)', '_return', '_result', '_[', '_0', '_]', '_+', '_level', '_+', '_result', '_[', '_1', '_]', '_.', '_split', '_(', '_";"', '_)', '_[', '_0', '_]', '</s>']
10/09/2022 07:08:46 - INFO - __main__ -   code_ids: 0 6 2 729 5192 181 3258 896 386 207 400 428 2019 3144 385 437 201 120 743 545 3144 385 3144 513 12945 120 1046 385 428 746 5192 400 3144 743 483 1046 626 461 2406 513 3144 513 1046 626 524 2406 746 5192 400 29760 743 626 461 2406 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:08:46 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Return', '_either', '_the', '_full', '_or', '_truncated', '_version', '_of', '_a', '_Q', 'II', 'ME', '_-', '_formatted', '_taxonomy', '_string', '_.', '</s>']
10/09/2022 07:08:46 - INFO - __main__ -   nl_ids: 0 6 2 1675 4759 448 3662 872 19307 2229 595 434 1152 4300 1098 581 10440 29021 724 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:08:46 - INFO - __main__ -   *** Example ***
10/09/2022 07:08:46 - INFO - __main__ -   idx: 1
10/09/2022 07:08:46 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_ensure', '_', 'dir', '_(', '_d', '_)', '_:', '_if', '_not', '_os', '_.', '_path', '_.', '_exists', '_(', '_d', '_)', '_:', '_try', '_:', '_os', '_.', '_m', 'akedirs', '_(', '_d', '_)', '_except', '_OSError', '_as', '_oe', '_:', '_#', '_should', '_not', '_happen', '_with', '_os', '.', 'makedirs', '_#', '_ENOENT', ':', '_No', '_such', '_file', '_or', '_directory', '_if', '_os', '_.', '_errno', '_==', '_errno', '_.', '_ENOENT', '_:', '_msg', '_=', '_tw', 'dd', '_(', '_"""', 'One', '_or', '_more', '_directories', '_in', '_the', '_path', '_({})', '_do', '_not', '_exist', '.', '_If', 'Ċ', '__________________________', '_you', '_are', '_specifying', '_a', '_new', '_directory', '_for', '_output', ',', '_please', '_ensure', 'Ċ', '__________________________', '_all', '_other', '_directories', '_in', '_the', '_path', '_currently', '_exist', '."""', '_)', '_return', '_msg', '_.', '_format', '_(', '_d', '_)', '_else', '_:', '_msg', '_=', '_tw', 'dd', '_(', '_"""', 'An', '_error', '_occurred', '_trying', '_to', '_create', '_the', '_output', '_directory', 'Ċ', '__________________________', '_({})', '_with', '_message', ':', '_{}', '"""', '_)', '_return', '_msg', '_.', '_format', '_(', '_d', '_,', '_oe', '_.', '_strerror', '_)', '</s>']
10/09/2022 07:08:46 - INFO - __main__ -   code_ids: 0 6 2 729 6229 181 1282 400 480 743 545 462 800 2215 746 1391 746 4534 400 480 743 545 1568 545 2215 746 446 23328 400 480 743 3552 22934 880 44902 545 830 1570 800 7564 918 2215 132 24429 830 41059 144 4038 5632 1012 872 3456 462 2215 746 2341 550 2341 746 41059 545 2345 385 7916 443 400 1638 3533 872 2726 11613 488 448 1391 46072 1000 800 3040 132 1359 317 4584 2713 1147 15323 434 579 3456 563 1721 130 13874 6229 317 4584 1345 1946 11613 488 448 1391 6418 3040 6315 743 483 2345 746 2021 400 480 743 669 545 2345 385 7916 443 400 1638 1088 843 10058 11749 508 1738 448 1721 3456 317 4584 46072 918 1841 144 2334 3947 743 483 2345 746 2021 400 480 2019 44902 746 20115 743 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:08:46 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Check', '_to', '_make', '_sure', '_the', '_supplied', '_directory', '_path', '_does', '_not', '_exist', '_if', '_so', '_create', '_it', '_.', '_The', '_method', '_catch', 'es', '_OSError', '_exceptions', '_and', '_returns', '_a', '_desc', 'riptive', '_message', '_instead', '_of', '_re', '_-', '_raising', '_the', '_error', '_.', '</s>']
10/09/2022 07:08:46 - INFO - __main__ -   nl_ids: 0 6 2 1749 508 2002 3984 448 8813 3456 1391 2129 800 3040 462 1769 1738 835 746 1044 1454 2092 482 22934 12300 706 2060 434 2162 44105 1841 4488 595 479 581 47183 448 843 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:08:46 - INFO - __main__ -   *** Example ***
10/09/2022 07:08:46 - INFO - __main__ -   idx: 2
10/09/2022 07:08:46 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_file', '_', 'handle', '_(', '_fn', 'h', '_,', '_mode', '_=', '_"', 'r', 'U', '"', '_)', '_:', '_handle', '_=', '_None', '_if', '_isinstance', '_(', '_fn', 'h', '_,', '_file', '_)', '_:', '_if', '_fn', 'h', '_.', '_closed', '_:', '_raise', '_ValueError', '_(', '_"', 'Input', '_file', '_is', '_closed', '."', '_)', '_handle', '_=', '_fn', 'h', '_elif', '_isinstance', '_(', '_fn', 'h', '_,', '_str', '_)', '_:', '_handle', '_=', '_open', '_(', '_fn', 'h', '_,', '_mode', '_)', '_return', '_handle', '</s>']
10/09/2022 07:08:46 - INFO - __main__ -   code_ids: 0 6 2 729 1012 181 2133 400 4065 190 2019 2119 385 437 200 171 120 743 545 2384 385 1938 462 5408 400 4065 190 2019 1012 743 545 462 4065 190 746 8264 545 3085 6052 400 437 1834 1012 555 8264 3508 743 2384 385 4065 190 3625 5408 400 4065 190 2019 1113 743 545 2384 385 2717 400 4065 190 2019 2119 743 483 2384 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:08:46 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Takes', '_either', '_a', '_file', '_path', '_or', '_an', '_open', '_file', '_handle', '_checks', '_validity', '_and', '_returns', '_an', '_open', '_file', '_handle', '_or', '_raises', '_an', '_appropriate', '_Exception', '_.', '</s>']
10/09/2022 07:08:46 - INFO - __main__ -   nl_ids: 0 6 2 27408 4759 434 1012 1391 872 817 2717 1012 2384 7825 25911 706 2060 817 2717 1012 2384 872 23154 817 7900 2654 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:08:47 - INFO - __main__ -   ***** Running training *****
10/09/2022 07:08:47 - INFO - __main__ -     Num examples = 251820
10/09/2022 07:08:47 - INFO - __main__ -     Num Epochs = 10
10/09/2022 07:08:47 - INFO - __main__ -     Instantaneous batch size per GPU = 128
10/09/2022 07:08:47 - INFO - __main__ -     Total train batch size  = 128
10/09/2022 07:08:47 - INFO - __main__ -     Total optimization steps = 19680
10/09/2022 07:09:34 - INFO - __main__ -   epoch 0 step 100 loss 0.20898
10/09/2022 07:10:18 - INFO - __main__ -   epoch 0 step 200 loss 0.1524
10/09/2022 07:11:02 - INFO - __main__ -   epoch 0 step 300 loss 0.14129
10/09/2022 07:11:46 - INFO - __main__ -   epoch 0 step 400 loss 0.13335
10/09/2022 07:12:30 - INFO - __main__ -   epoch 0 step 500 loss 0.1385
10/09/2022 07:13:15 - INFO - __main__ -   epoch 0 step 600 loss 0.13283
10/09/2022 07:13:59 - INFO - __main__ -   epoch 0 step 700 loss 0.12958
10/09/2022 07:14:43 - INFO - __main__ -   epoch 0 step 800 loss 0.12026
10/09/2022 07:15:27 - INFO - __main__ -   epoch 0 step 900 loss 0.11878
10/09/2022 07:16:11 - INFO - __main__ -   epoch 0 step 1000 loss 0.1116
10/09/2022 07:16:55 - INFO - __main__ -   epoch 0 step 1100 loss 0.10791
10/09/2022 07:17:39 - INFO - __main__ -   epoch 0 step 1200 loss 0.12007
10/09/2022 07:18:23 - INFO - __main__ -   epoch 0 step 1300 loss 0.10277
10/09/2022 07:19:08 - INFO - __main__ -   epoch 0 step 1400 loss 0.11565
10/09/2022 07:19:52 - INFO - __main__ -   epoch 0 step 1500 loss 0.11586
10/09/2022 07:20:36 - INFO - __main__ -   epoch 0 step 1600 loss 0.10921
10/09/2022 07:21:20 - INFO - __main__ -   epoch 0 step 1700 loss 0.11011
10/09/2022 07:22:04 - INFO - __main__ -   epoch 0 step 1800 loss 0.10625
10/09/2022 07:22:48 - INFO - __main__ -   epoch 0 step 1900 loss 0.11477
10/09/2022 07:23:58 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:23:58 - INFO - __main__ -     Num queries = 13914
10/09/2022 07:23:58 - INFO - __main__ -     Num codes = 43827
10/09/2022 07:23:58 - INFO - __main__ -     Batch size = 128
10/09/2022 07:25:49 - INFO - __main__ -     R@1 = 0.612
10/09/2022 07:25:49 - INFO - __main__ -     R@5 = 0.831
10/09/2022 07:25:49 - INFO - __main__ -     R@10 = 0.882
10/09/2022 07:25:49 - INFO - __main__ -     eval_mrr = 0.711
10/09/2022 07:25:49 - INFO - __main__ -     ********************
10/09/2022 07:25:49 - INFO - __main__ -     Best mrr:0.711
10/09/2022 07:25:49 - INFO - __main__ -     ********************
10/09/2022 07:25:54 - INFO - __main__ -   Saving model checkpoint to saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_3_layers/20221009070453/checkpoint-best-mrr/model.bin
10/09/2022 07:26:39 - INFO - __main__ -   epoch 1 step 100 loss 0.08945
10/09/2022 07:27:23 - INFO - __main__ -   epoch 1 step 200 loss 0.0819
10/09/2022 07:28:07 - INFO - __main__ -   epoch 1 step 300 loss 0.07744
10/09/2022 07:28:51 - INFO - __main__ -   epoch 1 step 400 loss 0.07818
10/09/2022 07:29:35 - INFO - __main__ -   epoch 1 step 500 loss 0.08089
10/09/2022 07:30:19 - INFO - __main__ -   epoch 1 step 600 loss 0.08236
10/09/2022 07:31:03 - INFO - __main__ -   epoch 1 step 700 loss 0.07545
10/09/2022 07:31:48 - INFO - __main__ -   epoch 1 step 800 loss 0.08579
10/09/2022 07:32:32 - INFO - __main__ -   epoch 1 step 900 loss 0.08486
10/09/2022 07:33:16 - INFO - __main__ -   epoch 1 step 1000 loss 0.0836
10/09/2022 07:34:00 - INFO - __main__ -   epoch 1 step 1100 loss 0.08325
10/09/2022 07:34:44 - INFO - __main__ -   epoch 1 step 1200 loss 0.07829
10/09/2022 07:35:28 - INFO - __main__ -   epoch 1 step 1300 loss 0.07947
10/09/2022 07:36:12 - INFO - __main__ -   epoch 1 step 1400 loss 0.07849
10/09/2022 07:36:56 - INFO - __main__ -   epoch 1 step 1500 loss 0.07519
10/09/2022 07:37:40 - INFO - __main__ -   epoch 1 step 1600 loss 0.07792
10/09/2022 07:38:24 - INFO - __main__ -   epoch 1 step 1700 loss 0.07955
10/09/2022 07:39:08 - INFO - __main__ -   epoch 1 step 1800 loss 0.08025
10/09/2022 07:39:52 - INFO - __main__ -   epoch 1 step 1900 loss 0.07751
10/09/2022 07:40:57 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:40:57 - INFO - __main__ -     Num queries = 13914
10/09/2022 07:40:57 - INFO - __main__ -     Num codes = 43827
10/09/2022 07:40:57 - INFO - __main__ -     Batch size = 128
10/09/2022 07:42:49 - INFO - __main__ -     R@1 = 0.617
10/09/2022 07:42:49 - INFO - __main__ -     R@5 = 0.834
10/09/2022 07:42:49 - INFO - __main__ -     R@10 = 0.887
10/09/2022 07:42:49 - INFO - __main__ -     eval_mrr = 0.714
10/09/2022 07:42:49 - INFO - __main__ -     ********************
10/09/2022 07:42:49 - INFO - __main__ -     Best mrr:0.714
10/09/2022 07:42:49 - INFO - __main__ -     ********************
10/09/2022 07:42:58 - INFO - __main__ -   Saving model checkpoint to saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_3_layers/20221009070453/checkpoint-best-mrr/model.bin
10/09/2022 07:43:43 - INFO - __main__ -   epoch 2 step 100 loss 0.06346
10/09/2022 07:44:27 - INFO - __main__ -   epoch 2 step 200 loss 0.05433
10/09/2022 07:45:11 - INFO - __main__ -   epoch 2 step 300 loss 0.05564
10/09/2022 07:45:55 - INFO - __main__ -   epoch 2 step 400 loss 0.05527
10/09/2022 07:46:39 - INFO - __main__ -   epoch 2 step 500 loss 0.05894
10/09/2022 07:47:23 - INFO - __main__ -   epoch 2 step 600 loss 0.05928
10/09/2022 07:48:07 - INFO - __main__ -   epoch 2 step 700 loss 0.05727
10/09/2022 07:48:51 - INFO - __main__ -   epoch 2 step 800 loss 0.05452
10/09/2022 07:49:35 - INFO - __main__ -   epoch 2 step 900 loss 0.06295
10/09/2022 07:50:19 - INFO - __main__ -   epoch 2 step 1000 loss 0.05852
10/09/2022 07:51:03 - INFO - __main__ -   epoch 2 step 1100 loss 0.05972
10/09/2022 07:51:47 - INFO - __main__ -   epoch 2 step 1200 loss 0.06028
10/09/2022 07:52:31 - INFO - __main__ -   epoch 2 step 1300 loss 0.06201
10/09/2022 07:53:15 - INFO - __main__ -   epoch 2 step 1400 loss 0.06206
10/09/2022 07:53:59 - INFO - __main__ -   epoch 2 step 1500 loss 0.05741
10/09/2022 07:54:43 - INFO - __main__ -   epoch 2 step 1600 loss 0.06149
10/09/2022 07:55:27 - INFO - __main__ -   epoch 2 step 1700 loss 0.05938
10/09/2022 07:56:11 - INFO - __main__ -   epoch 2 step 1800 loss 0.05932
10/09/2022 07:56:55 - INFO - __main__ -   epoch 2 step 1900 loss 0.06012
10/09/2022 07:57:59 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:57:59 - INFO - __main__ -     Num queries = 13914
10/09/2022 07:57:59 - INFO - __main__ -     Num codes = 43827
10/09/2022 07:57:59 - INFO - __main__ -     Batch size = 128
10/09/2022 07:59:50 - INFO - __main__ -     R@1 = 0.619
10/09/2022 07:59:50 - INFO - __main__ -     R@5 = 0.836
10/09/2022 07:59:50 - INFO - __main__ -     R@10 = 0.886
10/09/2022 07:59:50 - INFO - __main__ -     eval_mrr = 0.716
10/09/2022 07:59:50 - INFO - __main__ -     ********************
10/09/2022 07:59:50 - INFO - __main__ -     Best mrr:0.716
10/09/2022 07:59:50 - INFO - __main__ -     ********************
10/09/2022 07:59:58 - INFO - __main__ -   Saving model checkpoint to saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_3_layers/20221009070453/checkpoint-best-mrr/model.bin
10/09/2022 08:00:42 - INFO - __main__ -   epoch 3 step 100 loss 0.05093
10/09/2022 08:01:26 - INFO - __main__ -   epoch 3 step 200 loss 0.04269
10/09/2022 08:02:10 - INFO - __main__ -   epoch 3 step 300 loss 0.04633
10/09/2022 08:02:54 - INFO - __main__ -   epoch 3 step 400 loss 0.04581
10/09/2022 08:03:39 - INFO - __main__ -   epoch 3 step 500 loss 0.04702
10/09/2022 08:04:23 - INFO - __main__ -   epoch 3 step 600 loss 0.04434
10/09/2022 08:05:07 - INFO - __main__ -   epoch 3 step 700 loss 0.04658
10/09/2022 08:05:51 - INFO - __main__ -   epoch 3 step 800 loss 0.04543
10/09/2022 08:06:35 - INFO - __main__ -   epoch 3 step 900 loss 0.04653
10/09/2022 08:07:19 - INFO - __main__ -   epoch 3 step 1000 loss 0.04634
10/09/2022 08:08:03 - INFO - __main__ -   epoch 3 step 1100 loss 0.04328
10/09/2022 08:08:47 - INFO - __main__ -   epoch 3 step 1200 loss 0.04615
10/09/2022 08:09:31 - INFO - __main__ -   epoch 3 step 1300 loss 0.04664
10/09/2022 08:10:15 - INFO - __main__ -   epoch 3 step 1400 loss 0.04547
10/09/2022 08:10:59 - INFO - __main__ -   epoch 3 step 1500 loss 0.04057
10/09/2022 08:11:43 - INFO - __main__ -   epoch 3 step 1600 loss 0.04596
10/09/2022 08:12:27 - INFO - __main__ -   epoch 3 step 1700 loss 0.04411
10/09/2022 08:13:11 - INFO - __main__ -   epoch 3 step 1800 loss 0.04427
10/09/2022 08:13:55 - INFO - __main__ -   epoch 3 step 1900 loss 0.04709
10/09/2022 08:14:59 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 08:14:59 - INFO - __main__ -     Num queries = 13914
10/09/2022 08:14:59 - INFO - __main__ -     Num codes = 43827
10/09/2022 08:14:59 - INFO - __main__ -     Batch size = 128
10/09/2022 08:16:51 - INFO - __main__ -     R@1 = 0.613
10/09/2022 08:16:51 - INFO - __main__ -     R@5 = 0.833
10/09/2022 08:16:51 - INFO - __main__ -     R@10 = 0.886
10/09/2022 08:16:51 - INFO - __main__ -     eval_mrr = 0.712
10/09/2022 08:17:35 - INFO - __main__ -   epoch 4 step 100 loss 0.03771
10/09/2022 08:18:19 - INFO - __main__ -   epoch 4 step 200 loss 0.03411
10/09/2022 08:19:04 - INFO - __main__ -   epoch 4 step 300 loss 0.03882
10/09/2022 08:19:48 - INFO - __main__ -   epoch 4 step 400 loss 0.03729
10/09/2022 08:20:32 - INFO - __main__ -   epoch 4 step 500 loss 0.03721
10/09/2022 08:21:16 - INFO - __main__ -   epoch 4 step 600 loss 0.03727
10/09/2022 08:22:00 - INFO - __main__ -   epoch 4 step 700 loss 0.03622
10/09/2022 08:22:44 - INFO - __main__ -   epoch 4 step 800 loss 0.03557
10/09/2022 08:23:28 - INFO - __main__ -   epoch 4 step 900 loss 0.03583
10/09/2022 08:24:12 - INFO - __main__ -   epoch 4 step 1000 loss 0.03647
10/09/2022 08:24:56 - INFO - __main__ -   epoch 4 step 1100 loss 0.03434
10/09/2022 08:25:40 - INFO - __main__ -   epoch 4 step 1200 loss 0.03737
10/09/2022 08:26:24 - INFO - __main__ -   epoch 4 step 1300 loss 0.03705
10/09/2022 08:27:08 - INFO - __main__ -   epoch 4 step 1400 loss 0.03576
10/09/2022 08:27:52 - INFO - __main__ -   epoch 4 step 1500 loss 0.0375
10/09/2022 08:28:36 - INFO - __main__ -   epoch 4 step 1600 loss 0.03771
10/09/2022 08:29:20 - INFO - __main__ -   epoch 4 step 1700 loss 0.03566
10/09/2022 08:30:04 - INFO - __main__ -   epoch 4 step 1800 loss 0.03911
10/09/2022 08:30:48 - INFO - __main__ -   epoch 4 step 1900 loss 0.03676
10/09/2022 08:31:52 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 08:31:52 - INFO - __main__ -     Num queries = 13914
10/09/2022 08:31:52 - INFO - __main__ -     Num codes = 43827
10/09/2022 08:31:52 - INFO - __main__ -     Batch size = 128
10/09/2022 08:33:43 - INFO - __main__ -     R@1 = 0.614
10/09/2022 08:33:43 - INFO - __main__ -     R@5 = 0.836
10/09/2022 08:33:43 - INFO - __main__ -     R@10 = 0.885
10/09/2022 08:33:43 - INFO - __main__ -     eval_mrr = 0.713
10/09/2022 08:34:28 - INFO - __main__ -   epoch 5 step 100 loss 0.0314
10/09/2022 08:35:12 - INFO - __main__ -   epoch 5 step 200 loss 0.03038
10/09/2022 08:35:56 - INFO - __main__ -   epoch 5 step 300 loss 0.03047
10/09/2022 08:36:40 - INFO - __main__ -   epoch 5 step 400 loss 0.02979
10/09/2022 08:37:24 - INFO - __main__ -   epoch 5 step 500 loss 0.02992
10/09/2022 08:38:07 - INFO - __main__ -   epoch 5 step 600 loss 0.03035
10/09/2022 08:38:51 - INFO - __main__ -   epoch 5 step 700 loss 0.02911
10/09/2022 08:39:35 - INFO - __main__ -   epoch 5 step 800 loss 0.03207
10/09/2022 08:40:19 - INFO - __main__ -   epoch 5 step 900 loss 0.03252
10/09/2022 08:41:03 - INFO - __main__ -   epoch 5 step 1000 loss 0.03019
10/09/2022 08:41:47 - INFO - __main__ -   epoch 5 step 1100 loss 0.02979
10/09/2022 08:42:31 - INFO - __main__ -   epoch 5 step 1200 loss 0.0285
10/09/2022 08:43:15 - INFO - __main__ -   epoch 5 step 1300 loss 0.03128
10/09/2022 08:43:59 - INFO - __main__ -   epoch 5 step 1400 loss 0.0302
10/09/2022 08:44:43 - INFO - __main__ -   epoch 5 step 1500 loss 0.03192
10/09/2022 08:45:27 - INFO - __main__ -   epoch 5 step 1600 loss 0.03225
10/09/2022 08:46:11 - INFO - __main__ -   epoch 5 step 1700 loss 0.03025
10/09/2022 08:46:55 - INFO - __main__ -   epoch 5 step 1800 loss 0.0334
10/09/2022 08:47:39 - INFO - __main__ -   epoch 5 step 1900 loss 0.03274
10/09/2022 08:48:43 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 08:48:43 - INFO - __main__ -     Num queries = 13914
10/09/2022 08:48:43 - INFO - __main__ -     Num codes = 43827
10/09/2022 08:48:43 - INFO - __main__ -     Batch size = 128
10/09/2022 08:50:35 - INFO - __main__ -     R@1 = 0.617
10/09/2022 08:50:35 - INFO - __main__ -     R@5 = 0.836
10/09/2022 08:50:35 - INFO - __main__ -     R@10 = 0.887
10/09/2022 08:50:35 - INFO - __main__ -     eval_mrr = 0.714
10/09/2022 08:51:19 - INFO - __main__ -   epoch 6 step 100 loss 0.02859
10/09/2022 08:52:03 - INFO - __main__ -   epoch 6 step 200 loss 0.02855
10/09/2022 08:52:47 - INFO - __main__ -   epoch 6 step 300 loss 0.02634
10/09/2022 08:53:31 - INFO - __main__ -   epoch 6 step 400 loss 0.026
10/09/2022 08:54:15 - INFO - __main__ -   epoch 6 step 500 loss 0.02666
10/09/2022 08:54:59 - INFO - __main__ -   epoch 6 step 600 loss 0.02746
10/09/2022 08:55:43 - INFO - __main__ -   epoch 6 step 700 loss 0.0249
10/09/2022 08:56:27 - INFO - __main__ -   epoch 6 step 800 loss 0.0261
10/09/2022 08:57:11 - INFO - __main__ -   epoch 6 step 900 loss 0.02617
10/09/2022 08:57:55 - INFO - __main__ -   epoch 6 step 1000 loss 0.02682
10/09/2022 08:58:39 - INFO - __main__ -   epoch 6 step 1100 loss 0.02662
10/09/2022 08:59:23 - INFO - __main__ -   epoch 6 step 1200 loss 0.02563
10/09/2022 09:00:07 - INFO - __main__ -   epoch 6 step 1300 loss 0.0252
10/09/2022 09:00:51 - INFO - __main__ -   epoch 6 step 1400 loss 0.02891
10/09/2022 09:01:35 - INFO - __main__ -   epoch 6 step 1500 loss 0.0284
10/09/2022 09:02:19 - INFO - __main__ -   epoch 6 step 1600 loss 0.02689
10/09/2022 09:03:03 - INFO - __main__ -   epoch 6 step 1700 loss 0.02728
10/09/2022 09:03:47 - INFO - __main__ -   epoch 6 step 1800 loss 0.02735
10/09/2022 09:04:31 - INFO - __main__ -   epoch 6 step 1900 loss 0.02749
10/09/2022 09:05:36 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 09:05:36 - INFO - __main__ -     Num queries = 13914
10/09/2022 09:05:36 - INFO - __main__ -     Num codes = 43827
10/09/2022 09:05:36 - INFO - __main__ -     Batch size = 128
10/09/2022 09:07:27 - INFO - __main__ -     R@1 = 0.615
10/09/2022 09:07:27 - INFO - __main__ -     R@5 = 0.836
10/09/2022 09:07:27 - INFO - __main__ -     R@10 = 0.888
10/09/2022 09:07:27 - INFO - __main__ -     eval_mrr = 0.714
10/09/2022 09:08:11 - INFO - __main__ -   epoch 7 step 100 loss 0.02543
10/09/2022 09:08:55 - INFO - __main__ -   epoch 7 step 200 loss 0.02495
10/09/2022 09:09:39 - INFO - __main__ -   epoch 7 step 300 loss 0.02386
10/09/2022 09:10:23 - INFO - __main__ -   epoch 7 step 400 loss 0.02497
10/09/2022 09:11:07 - INFO - __main__ -   epoch 7 step 500 loss 0.02394
10/09/2022 09:11:51 - INFO - __main__ -   epoch 7 step 600 loss 0.02587
10/09/2022 09:12:35 - INFO - __main__ -   epoch 7 step 700 loss 0.02476
10/09/2022 09:13:19 - INFO - __main__ -   epoch 7 step 800 loss 0.02318
10/09/2022 09:14:03 - INFO - __main__ -   epoch 7 step 900 loss 0.02344
10/09/2022 09:14:47 - INFO - __main__ -   epoch 7 step 1000 loss 0.02373
10/09/2022 09:15:31 - INFO - __main__ -   epoch 7 step 1100 loss 0.02583
10/09/2022 09:16:15 - INFO - __main__ -   epoch 7 step 1200 loss 0.02387
10/09/2022 09:16:59 - INFO - __main__ -   epoch 7 step 1300 loss 0.02487
10/09/2022 09:17:43 - INFO - __main__ -   epoch 7 step 1400 loss 0.02584
10/09/2022 09:18:27 - INFO - __main__ -   epoch 7 step 1500 loss 0.02341
10/09/2022 09:19:11 - INFO - __main__ -   epoch 7 step 1600 loss 0.02462
10/09/2022 09:19:55 - INFO - __main__ -   epoch 7 step 1700 loss 0.02292
10/09/2022 09:20:39 - INFO - __main__ -   epoch 7 step 1800 loss 0.02525
10/09/2022 09:21:23 - INFO - __main__ -   epoch 7 step 1900 loss 0.02431
10/09/2022 09:22:27 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 09:22:27 - INFO - __main__ -     Num queries = 13914
10/09/2022 09:22:27 - INFO - __main__ -     Num codes = 43827
10/09/2022 09:22:27 - INFO - __main__ -     Batch size = 128
10/09/2022 09:24:20 - INFO - __main__ -     R@1 = 0.617
10/09/2022 09:24:20 - INFO - __main__ -     R@5 = 0.836
10/09/2022 09:24:20 - INFO - __main__ -     R@10 = 0.887
10/09/2022 09:24:20 - INFO - __main__ -     eval_mrr = 0.715
10/09/2022 09:25:05 - INFO - __main__ -   epoch 8 step 100 loss 0.02323
10/09/2022 09:25:49 - INFO - __main__ -   epoch 8 step 200 loss 0.02181
10/09/2022 09:26:33 - INFO - __main__ -   epoch 8 step 300 loss 0.02329
10/09/2022 09:27:17 - INFO - __main__ -   epoch 8 step 400 loss 0.02198
10/09/2022 09:28:01 - INFO - __main__ -   epoch 8 step 500 loss 0.02249
10/09/2022 09:28:45 - INFO - __main__ -   epoch 8 step 600 loss 0.02351
10/09/2022 09:29:29 - INFO - __main__ -   epoch 8 step 700 loss 0.02162
10/09/2022 09:30:13 - INFO - __main__ -   epoch 8 step 800 loss 0.02314
10/09/2022 09:30:57 - INFO - __main__ -   epoch 8 step 900 loss 0.02345
10/09/2022 09:31:41 - INFO - __main__ -   epoch 8 step 1000 loss 0.02293
10/09/2022 09:32:25 - INFO - __main__ -   epoch 8 step 1100 loss 0.02256
10/09/2022 09:33:09 - INFO - __main__ -   epoch 8 step 1200 loss 0.02423
10/09/2022 09:33:53 - INFO - __main__ -   epoch 8 step 1300 loss 0.02354
10/09/2022 09:34:37 - INFO - __main__ -   epoch 8 step 1400 loss 0.02225
10/09/2022 09:35:21 - INFO - __main__ -   epoch 8 step 1500 loss 0.02364
10/09/2022 09:36:05 - INFO - __main__ -   epoch 8 step 1600 loss 0.02115
10/09/2022 09:36:49 - INFO - __main__ -   epoch 8 step 1700 loss 0.0225
10/09/2022 09:37:33 - INFO - __main__ -   epoch 8 step 1800 loss 0.02312
10/09/2022 09:38:17 - INFO - __main__ -   epoch 8 step 1900 loss 0.02339
10/09/2022 09:39:19 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 09:39:19 - INFO - __main__ -     Num queries = 13914
10/09/2022 09:39:19 - INFO - __main__ -     Num codes = 43827
10/09/2022 09:39:19 - INFO - __main__ -     Batch size = 128
10/09/2022 09:41:10 - INFO - __main__ -     R@1 = 0.619
10/09/2022 09:41:10 - INFO - __main__ -     R@5 = 0.837
10/09/2022 09:41:10 - INFO - __main__ -     R@10 = 0.888
10/09/2022 09:41:10 - INFO - __main__ -     eval_mrr = 0.716
10/09/2022 09:41:57 - INFO - __main__ -   epoch 9 step 100 loss 0.02348
10/09/2022 09:42:41 - INFO - __main__ -   epoch 9 step 200 loss 0.02222
10/09/2022 09:43:25 - INFO - __main__ -   epoch 9 step 300 loss 0.02351
10/09/2022 09:44:09 - INFO - __main__ -   epoch 9 step 400 loss 0.02193
10/09/2022 09:44:53 - INFO - __main__ -   epoch 9 step 500 loss 0.02249
10/09/2022 09:45:37 - INFO - __main__ -   epoch 9 step 600 loss 0.02068
10/09/2022 09:46:21 - INFO - __main__ -   epoch 9 step 700 loss 0.02066
10/09/2022 09:47:05 - INFO - __main__ -   epoch 9 step 800 loss 0.02195
10/09/2022 09:47:49 - INFO - __main__ -   epoch 9 step 900 loss 0.02214
10/09/2022 09:48:33 - INFO - __main__ -   epoch 9 step 1000 loss 0.02187
10/09/2022 09:49:17 - INFO - __main__ -   epoch 9 step 1100 loss 0.02094
10/09/2022 09:50:01 - INFO - __main__ -   epoch 9 step 1200 loss 0.02156
10/09/2022 09:50:45 - INFO - __main__ -   epoch 9 step 1300 loss 0.02138
10/09/2022 09:51:29 - INFO - __main__ -   epoch 9 step 1400 loss 0.02156
10/09/2022 09:52:13 - INFO - __main__ -   epoch 9 step 1500 loss 0.02202
10/09/2022 09:52:57 - INFO - __main__ -   epoch 9 step 1600 loss 0.02295
10/09/2022 09:53:41 - INFO - __main__ -   epoch 9 step 1700 loss 0.02104
10/09/2022 09:54:25 - INFO - __main__ -   epoch 9 step 1800 loss 0.02195
10/09/2022 09:55:09 - INFO - __main__ -   epoch 9 step 1900 loss 0.02062
10/09/2022 09:56:12 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 09:56:12 - INFO - __main__ -     Num queries = 13914
10/09/2022 09:56:12 - INFO - __main__ -     Num codes = 43827
10/09/2022 09:56:12 - INFO - __main__ -     Batch size = 128
10/09/2022 09:58:04 - INFO - __main__ -     R@1 = 0.619
10/09/2022 09:58:04 - INFO - __main__ -     R@5 = 0.836
10/09/2022 09:58:04 - INFO - __main__ -     R@10 = 0.887
10/09/2022 09:58:04 - INFO - __main__ -     eval_mrr = 0.716
10/09/2022 09:58:39 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 09:58:39 - INFO - __main__ -     Num queries = 13914
10/09/2022 09:58:39 - INFO - __main__ -     Num codes = 43827
10/09/2022 09:58:39 - INFO - __main__ -     Batch size = 128
10/09/2022 10:00:30 - INFO - __main__ -   ***** Eval results *****
10/09/2022 10:00:30 - INFO - __main__ -     R@1 = 0.619
10/09/2022 10:00:30 - INFO - __main__ -     R@10 = 0.886
10/09/2022 10:00:30 - INFO - __main__ -     R@5 = 0.836
10/09/2022 10:00:30 - INFO - __main__ -     eval_mrr = 0.716
10/09/2022 10:01:05 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 10:01:05 - INFO - __main__ -     Num queries = 14918
10/09/2022 10:01:05 - INFO - __main__ -     Num codes = 43827
10/09/2022 10:01:05 - INFO - __main__ -     Batch size = 128
10/09/2022 10:03:00 - INFO - __main__ -   ***** Eval results *****
10/09/2022 10:03:00 - INFO - __main__ -     R@1 = 0.626
10/09/2022 10:03:00 - INFO - __main__ -     R@10 = 0.895
10/09/2022 10:03:00 - INFO - __main__ -     R@5 = 0.842
10/09/2022 10:03:00 - INFO - __main__ -     eval_mrr = 0.724
10/09/2022 10:03:01 - INFO - utils -   saved dataset in saved_models/code_search/unixcoder/partial_freezing/python/freeze_bottom_3_layers/20221009070453/result.jsonl
