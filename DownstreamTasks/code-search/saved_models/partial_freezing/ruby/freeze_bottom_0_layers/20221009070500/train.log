10/09/2022 07:05:06 - INFO - __main__ -   device: cuda, n_gpu: 1
10/09/2022 07:05:07 - DEBUG - filelock -   Attempting to acquire lock 140102910983424 on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
10/09/2022 07:05:07 - DEBUG - filelock -   Lock 140102910983424 acquired on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
Downloading:   0%|          | 0.00/916k [00:00<?, ?B/s]Downloading:   4%|▍         | 37.0k/916k [00:00<00:02, 355kB/s]Downloading:  18%|█▊        | 169k/916k [00:00<00:00, 879kB/s] Downloading:  76%|███████▌  | 697k/916k [00:00<00:00, 2.75MB/s]Downloading: 100%|██████████| 916k/916k [00:00<00:00, 2.86MB/s]
10/09/2022 07:05:07 - DEBUG - filelock -   Attempting to release lock 140102910983424 on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
10/09/2022 07:05:07 - DEBUG - filelock -   Lock 140102910983424 released on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
10/09/2022 07:05:08 - DEBUG - filelock -   Attempting to acquire lock 140102910983280 on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
10/09/2022 07:05:08 - DEBUG - filelock -   Lock 140102910983280 acquired on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
Downloading:   0%|          | 0.00/434k [00:00<?, ?B/s]Downloading:   9%|▉         | 40.0k/434k [00:00<00:01, 377kB/s]Downloading:  45%|████▌     | 196k/434k [00:00<00:00, 1.02MB/s]Downloading: 100%|██████████| 434k/434k [00:00<00:00, 1.62MB/s]
10/09/2022 07:05:08 - DEBUG - filelock -   Attempting to release lock 140102910983280 on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
10/09/2022 07:05:08 - DEBUG - filelock -   Lock 140102910983280 released on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
10/09/2022 07:05:09 - DEBUG - filelock -   Attempting to acquire lock 140102910983280 on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
10/09/2022 07:05:09 - DEBUG - filelock -   Lock 140102910983280 acquired on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]Downloading: 100%|██████████| 772/772 [00:00<00:00, 438kB/s]
10/09/2022 07:05:09 - DEBUG - filelock -   Attempting to release lock 140102910983280 on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
10/09/2022 07:05:09 - DEBUG - filelock -   Lock 140102910983280 released on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
10/09/2022 07:05:09 - DEBUG - filelock -   Attempting to acquire lock 140102910566016 on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
10/09/2022 07:05:09 - DEBUG - filelock -   Lock 140102910566016 acquired on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
Downloading:   0%|          | 0.00/1.09k [00:00<?, ?B/s]Downloading: 100%|██████████| 1.09k/1.09k [00:00<00:00, 690kB/s]
10/09/2022 07:05:09 - DEBUG - filelock -   Attempting to release lock 140102910566016 on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
10/09/2022 07:05:09 - DEBUG - filelock -   Lock 140102910566016 released on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
10/09/2022 07:05:10 - DEBUG - filelock -   Attempting to acquire lock 140102910566160 on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
10/09/2022 07:05:10 - DEBUG - filelock -   Lock 140102910566160 acquired on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
Downloading:   0%|          | 0.00/691 [00:00<?, ?B/s]Downloading: 100%|██████████| 691/691 [00:00<00:00, 583kB/s]
10/09/2022 07:05:10 - DEBUG - filelock -   Attempting to release lock 140102910566160 on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
10/09/2022 07:05:10 - DEBUG - filelock -   Lock 140102910566160 released on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
10/09/2022 07:05:11 - DEBUG - filelock -   Attempting to acquire lock 140102910844352 on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
10/09/2022 07:05:11 - DEBUG - filelock -   Lock 140102910844352 acquired on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
Downloading:   0%|          | 0.00/480M [00:00<?, ?B/s]Downloading:   1%|▏         | 6.85M/480M [00:00<00:06, 71.9MB/s]Downloading:   3%|▎         | 16.6M/480M [00:00<00:05, 89.7MB/s]Downloading:   6%|▌         | 26.5M/480M [00:00<00:04, 96.3MB/s]Downloading:   8%|▊         | 36.3M/480M [00:00<00:04, 98.8MB/s]Downloading:  10%|▉         | 46.2M/480M [00:00<00:04, 100MB/s] Downloading:  12%|█▏        | 56.0M/480M [00:00<00:04, 101MB/s]Downloading:  14%|█▎        | 65.8M/480M [00:00<00:04, 102MB/s]Downloading:  16%|█▌        | 75.6M/480M [00:00<00:04, 102MB/s]Downloading:  18%|█▊        | 85.4M/480M [00:00<00:04, 102MB/s]Downloading:  20%|█▉        | 95.3M/480M [00:01<00:03, 102MB/s]Downloading:  22%|██▏       | 105M/480M [00:01<00:03, 103MB/s] Downloading:  24%|██▍       | 115M/480M [00:01<00:03, 104MB/s]Downloading:  26%|██▌       | 126M/480M [00:01<00:03, 105MB/s]Downloading:  28%|██▊       | 136M/480M [00:01<00:03, 106MB/s]Downloading:  30%|███       | 146M/480M [00:01<00:03, 106MB/s]Downloading:  33%|███▎      | 156M/480M [00:01<00:03, 106MB/s]Downloading:  35%|███▍      | 166M/480M [00:01<00:03, 105MB/s]Downloading:  37%|███▋      | 177M/480M [00:01<00:03, 105MB/s]Downloading:  39%|███▉      | 187M/480M [00:01<00:02, 105MB/s]Downloading:  41%|████      | 197M/480M [00:02<00:02, 105MB/s]Downloading:  43%|████▎     | 207M/480M [00:02<00:02, 105MB/s]Downloading:  45%|████▌     | 217M/480M [00:02<00:02, 105MB/s]Downloading:  47%|████▋     | 227M/480M [00:02<00:02, 106MB/s]Downloading:  49%|████▉     | 237M/480M [00:02<00:02, 105MB/s]Downloading:  51%|█████▏    | 247M/480M [00:02<00:02, 106MB/s]Downloading:  54%|█████▎    | 257M/480M [00:02<00:02, 106MB/s]Downloading:  56%|█████▌    | 268M/480M [00:02<00:02, 105MB/s]Downloading:  58%|█████▊    | 278M/480M [00:02<00:02, 105MB/s]Downloading:  60%|█████▉    | 288M/480M [00:02<00:01, 105MB/s]Downloading:  62%|██████▏   | 298M/480M [00:03<00:01, 104MB/s]Downloading:  64%|██████▍   | 308M/480M [00:03<00:01, 104MB/s]Downloading:  66%|██████▌   | 318M/480M [00:03<00:01, 105MB/s]Downloading:  68%|██████▊   | 328M/480M [00:03<00:01, 106MB/s]Downloading:  70%|███████   | 338M/480M [00:03<00:01, 106MB/s]Downloading:  72%|███████▏  | 348M/480M [00:03<00:01, 104MB/s]Downloading:  75%|███████▍  | 358M/480M [00:03<00:01, 104MB/s]Downloading:  77%|███████▋  | 368M/480M [00:03<00:01, 104MB/s]Downloading:  79%|███████▊  | 378M/480M [00:03<00:01, 104MB/s]Downloading:  81%|████████  | 388M/480M [00:03<00:00, 104MB/s]Downloading:  83%|████████▎ | 398M/480M [00:04<00:00, 104MB/s]Downloading:  85%|████████▍ | 408M/480M [00:04<00:00, 104MB/s]Downloading:  87%|████████▋ | 418M/480M [00:04<00:00, 105MB/s]Downloading:  89%|████████▉ | 428M/480M [00:04<00:00, 105MB/s]Downloading:  91%|█████████ | 438M/480M [00:04<00:00, 105MB/s]Downloading:  93%|█████████▎| 448M/480M [00:04<00:00, 104MB/s]Downloading:  95%|█████████▌| 458M/480M [00:04<00:00, 104MB/s]Downloading:  97%|█████████▋| 468M/480M [00:04<00:00, 104MB/s]Downloading:  99%|█████████▉| 478M/480M [00:04<00:00, 104MB/s]Downloading: 100%|██████████| 480M/480M [00:04<00:00, 104MB/s]
10/09/2022 07:05:16 - DEBUG - filelock -   Attempting to release lock 140102910844352 on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
10/09/2022 07:05:16 - DEBUG - filelock -   Lock 140102910844352 released on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
10/09/2022 07:05:18 - INFO - __main__ -   Training/evaluation parameters Namespace(code_length=256, codebase_file='dataset/CSN/ruby/codebase.jsonl', config_name='', debug=False, device=device(type='cuda'), do_F2_norm=False, do_eval=True, do_test=True, do_train=True, do_zero_shot=False, eval_batch_size=128, eval_data_file='dataset/CSN/ruby/valid.jsonl', freeze_bottom_k_layer_index=0, learning_rate=2e-05, max_grad_norm=1.0, model_name_or_path='microsoft/unixcoder-base', n_debug_samples=100, n_gpu=1, nl_length=128, num_train_epochs=10, output_dir='saved_models/code_search/unixcoder/partial_freezing/ruby/freeze_bottom_0_layers/20221009070500', seed=123456, test_data_file='dataset/CSN/ruby/test.jsonl', tokenizer_name='', train_batch_size=128, train_data_file='dataset/CSN/ruby/train.jsonl', weight_decay=0.01)
10/09/2022 07:05:18 - INFO - __main__ -   +------------------------------------------------------------+--------------+---------+
| Layer Name                                                 | Output Shape | Param # |
+------------------------------------------------------------+--------------+---------+
| encoder.encoder.layer.0.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.0.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.0.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.0.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.0.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.0.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.0.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.0.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.0.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.0.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.0.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.0.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.0.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.0.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.0.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.0.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.1.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.1.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.1.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.1.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.1.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.1.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.1.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.1.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.1.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.1.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.1.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.1.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.1.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.1.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.1.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.1.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.2.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.2.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.2.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.2.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.2.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.2.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.2.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.2.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.2.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.2.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.2.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.2.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.2.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.2.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.2.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.2.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.3.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.3.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.3.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.3.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.3.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.3.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.3.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.3.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.3.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.3.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.3.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.3.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.3.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.3.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.3.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.3.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.4.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.4.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.4.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.4.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.4.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.4.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.4.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.4.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.4.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.4.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.4.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.4.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.4.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.4.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.4.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.4.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.5.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.5.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.5.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.5.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.5.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.5.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.5.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.5.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.5.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.5.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.5.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.5.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.5.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.5.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.5.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.5.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.6.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.6.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.6.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.6.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.6.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.6.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.6.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.6.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.6.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.6.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.6.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.6.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.7.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.7.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.7.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.7.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.7.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.7.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.7.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.7.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.7.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.7.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.7.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.7.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.8.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.8.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.8.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.8.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.8.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.8.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.8.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.8.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.8.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.8.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.8.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.8.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.9.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.9.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.9.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.9.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.9.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.9.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.9.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.9.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.9.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.9.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.9.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.9.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.10.attention.self.query.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.self.query.bias         |        [768] |     768 |
| encoder.encoder.layer.10.attention.self.key.weight         |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.self.key.bias           |        [768] |     768 |
| encoder.encoder.layer.10.attention.self.value.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.self.value.bias         |        [768] |     768 |
| encoder.encoder.layer.10.attention.output.dense.weight     |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.output.dense.bias       |        [768] |     768 |
| encoder.encoder.layer.10.attention.output.LayerNorm.weight |        [768] |     768 |
| encoder.encoder.layer.10.attention.output.LayerNorm.bias   |        [768] |     768 |
| encoder.encoder.layer.10.intermediate.dense.weight         |  [3072, 768] | 2359296 |
| encoder.encoder.layer.10.intermediate.dense.bias           |       [3072] |    3072 |
| encoder.encoder.layer.10.output.dense.weight               |  [768, 3072] | 2359296 |
| encoder.encoder.layer.10.output.dense.bias                 |        [768] |     768 |
| encoder.encoder.layer.10.output.LayerNorm.weight           |        [768] |     768 |
| encoder.encoder.layer.10.output.LayerNorm.bias             |        [768] |     768 |
| encoder.encoder.layer.11.attention.self.query.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.self.query.bias         |        [768] |     768 |
| encoder.encoder.layer.11.attention.self.key.weight         |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.self.key.bias           |        [768] |     768 |
| encoder.encoder.layer.11.attention.self.value.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.self.value.bias         |        [768] |     768 |
| encoder.encoder.layer.11.attention.output.dense.weight     |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.output.dense.bias       |        [768] |     768 |
| encoder.encoder.layer.11.attention.output.LayerNorm.weight |        [768] |     768 |
| encoder.encoder.layer.11.attention.output.LayerNorm.bias   |        [768] |     768 |
| encoder.encoder.layer.11.intermediate.dense.weight         |  [3072, 768] | 2359296 |
| encoder.encoder.layer.11.intermediate.dense.bias           |       [3072] |    3072 |
| encoder.encoder.layer.11.output.dense.weight               |  [768, 3072] | 2359296 |
| encoder.encoder.layer.11.output.dense.bias                 |        [768] |     768 |
| encoder.encoder.layer.11.output.LayerNorm.weight           |        [768] |     768 |
| encoder.encoder.layer.11.output.LayerNorm.bias             |        [768] |     768 |
| encoder.pooler.dense.weight                                |   [768, 768] |  589824 |
| encoder.pooler.dense.bias                                  |        [768] |     768 |
+------------------------------------------------------------+--------------+---------+
10/09/2022 07:05:37 - INFO - __main__ -   *** Example ***
10/09/2022 07:05:37 - INFO - __main__ -   idx: 0
10/09/2022 07:05:37 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_render', '_', 'body', '_(', '_context', '_,', '_options', '_)', '_if', '_options', '_.', '_key', '?', '_(', '_:', 'partial', '_)', '_[', '_render', '_', 'partial', '_(', '_context', '_,', '_options', '_)', '_]', '_else', '_Streaming', 'Template', 'Renderer', '_.', '_new', '_(', '_@', 'lookup', '_', 'context', '_)', '_.', '_render', '_(', '_context', '_,', '_options', '_)', '_end', '_end', '</s>']
10/09/2022 07:05:37 - INFO - __main__ -   code_ids: 0 6 2 729 4342 181 1995 400 1552 2019 1466 743 462 1466 746 1129 149 400 545 7609 743 626 4342 181 7609 400 1552 2019 1466 743 2406 669 47128 3057 6412 746 579 400 890 4961 181 1499 743 746 4342 400 1552 2019 1466 743 1013 1013 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:05:37 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Render', '_but', '_returns', '_a', '_valid', '_R', 'ack', '_body', '_.', '_If', '_fib', 'ers', '_are', '_defined', '_we', '_return', '_a', '_streaming', '_body', '_that', '_renders', '_the', '_template', '_piece', '_by', '_piece', '_.', '</s>']
10/09/2022 07:05:37 - INFO - __main__ -   nl_ids: 0 6 2 3726 2107 2060 434 1976 821 598 3444 746 1359 24766 560 1147 3474 937 483 434 22676 3444 922 40840 448 3636 18781 1243 18781 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:05:37 - INFO - __main__ -   *** Example ***
10/09/2022 07:05:37 - INFO - __main__ -   idx: 1
10/09/2022 07:05:37 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_attribute', '_', 'missing', '_(', '_match', '_,', '_*', '_args', '_,', '_&', '_block', '_)', '___', 'send', '__', '_(', '_match', '_.', '_target', '_,', '_match', '_.', '_attr', '_', 'name', '_,', '_args', '_,', '_block', '_)', '_end', '</s>']
10/09/2022 07:05:37 - INFO - __main__ -   code_ids: 0 6 2 729 2416 181 8487 400 1655 2019 426 1822 2019 519 1818 743 1267 2414 876 400 1655 746 1744 2019 1655 746 3526 181 616 2019 1822 2019 1818 743 1013 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:05:37 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', '+', '_attribute', '_', 'missing', '_+', '_is', '_like', '_+', '_method', '_', 'missing', '_+', '_but', '_for', '_attributes', '_.', '_When', '_+', '_method', '_', 'missing', '_+', '_is', '_called', '_we', '_check', '_to', '_see', '_if', '_there', '_is', '_a', '_matching', '_attribute', '_method', '_.', '_If', '_so', '_we', '_tell', '_+', '_attribute', '_', 'missing', '_+', '_to', '_dispatch', '_the', '_attribute', '_.', '_This', '_method', '_can', '_be', '_overloaded', '_to', '_customize', '_the', '_behavior', '_.', '</s>']
10/09/2022 07:05:37 - INFO - __main__ -   nl_ids: 0 6 2 129 2416 181 8487 513 555 4401 513 1454 181 8487 513 2107 563 4402 746 5919 513 1454 181 8487 513 555 2953 937 1382 508 3986 462 2550 555 434 6506 2416 1454 746 1359 1769 937 11931 513 2416 181 8487 513 508 9363 448 2416 746 1600 1454 1347 661 45869 508 36145 448 9050 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:05:37 - INFO - __main__ -   *** Example ***
10/09/2022 07:05:37 - INFO - __main__ -   idx: 2
10/09/2022 07:05:37 - INFO - __main__ -   code_tokens: ['<s>', '<encoder-only>', '</s>', 'def', '_matched', '_', 'attribute', '_', 'method', '_(', '_method', '_', 'name', '_)', '_matches', '_=', '_self', '_.', '_class', '_.', '_send', '_(', '_:', 'attribute', '_', 'method', '_', 'matchers', '_', 'matching', '_,', '_method', '_', 'name', '_)', '_matches', '_.', '_detect', '_{', '_|', '_match', '_|', '_attribute', '_', 'method', '?', '_(', '_match', '_.', '_attr', '_', 'name', '_)', '_}', '_end', '</s>']
10/09/2022 07:05:37 - INFO - __main__ -   code_ids: 0 6 2 729 5865 181 2163 181 1521 400 1454 181 616 743 5288 385 1358 746 1503 746 2904 400 545 2163 181 1521 181 38734 181 13575 2019 1454 181 616 743 5288 746 10241 399 649 1655 649 2416 181 1521 149 400 1655 746 3526 181 616 743 425 1013 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:05:37 - INFO - __main__ -   nl_tokens: ['<s>', '<encoder-only>', '</s>', 'Returns', '_a', '_struct', '_representing', '_the', '_matching', '_attribute', '_method', '_.', '_The', '_struct', '_s', '_attributes', '_are', '_prefix', '_base', '_and', '_suffix', '_.', '</s>']
10/09/2022 07:05:37 - INFO - __main__ -   nl_ids: 0 6 2 2853 434 1277 8466 448 6506 2416 1454 746 1044 1277 431 4402 1147 3603 1712 706 8436 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/09/2022 07:05:37 - INFO - __main__ -   ***** Running training *****
10/09/2022 07:05:37 - INFO - __main__ -     Num examples = 24927
10/09/2022 07:05:37 - INFO - __main__ -     Num Epochs = 10
10/09/2022 07:05:37 - INFO - __main__ -     Instantaneous batch size per GPU = 128
10/09/2022 07:05:37 - INFO - __main__ -     Total train batch size  = 128
10/09/2022 07:05:37 - INFO - __main__ -     Total optimization steps = 1950
10/09/2022 07:06:31 - INFO - __main__ -   epoch 0 step 100 loss 0.36776
10/09/2022 07:07:24 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:07:24 - INFO - __main__ -     Num queries = 1400
10/09/2022 07:07:24 - INFO - __main__ -     Num codes = 4360
10/09/2022 07:07:24 - INFO - __main__ -     Batch size = 128
10/09/2022 07:07:31 - INFO - __main__ -     R@1 = 0.697
10/09/2022 07:07:31 - INFO - __main__ -     R@5 = 0.889
10/09/2022 07:07:31 - INFO - __main__ -     R@10 = 0.919
10/09/2022 07:07:31 - INFO - __main__ -     eval_mrr = 0.781
10/09/2022 07:07:31 - INFO - __main__ -     ********************
10/09/2022 07:07:31 - INFO - __main__ -     Best mrr:0.781
10/09/2022 07:07:31 - INFO - __main__ -     ********************
10/09/2022 07:07:39 - INFO - __main__ -   Saving model checkpoint to saved_models/code_search/unixcoder/partial_freezing/ruby/freeze_bottom_0_layers/20221009070500/checkpoint-best-mrr/model.bin
10/09/2022 07:08:32 - INFO - __main__ -   epoch 1 step 100 loss 0.25443
10/09/2022 07:09:24 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:09:24 - INFO - __main__ -     Num queries = 1400
10/09/2022 07:09:24 - INFO - __main__ -     Num codes = 4360
10/09/2022 07:09:24 - INFO - __main__ -     Batch size = 128
10/09/2022 07:09:31 - INFO - __main__ -     R@1 = 0.698
10/09/2022 07:09:31 - INFO - __main__ -     R@5 = 0.885
10/09/2022 07:09:31 - INFO - __main__ -     R@10 = 0.916
10/09/2022 07:09:31 - INFO - __main__ -     eval_mrr = 0.781
10/09/2022 07:10:23 - INFO - __main__ -   epoch 2 step 100 loss 0.16941
10/09/2022 07:11:16 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:11:16 - INFO - __main__ -     Num queries = 1400
10/09/2022 07:11:16 - INFO - __main__ -     Num codes = 4360
10/09/2022 07:11:16 - INFO - __main__ -     Batch size = 128
10/09/2022 07:11:23 - INFO - __main__ -     R@1 = 0.702
10/09/2022 07:11:23 - INFO - __main__ -     R@5 = 0.889
10/09/2022 07:11:23 - INFO - __main__ -     R@10 = 0.919
10/09/2022 07:11:23 - INFO - __main__ -     eval_mrr = 0.784
10/09/2022 07:11:23 - INFO - __main__ -     ********************
10/09/2022 07:11:23 - INFO - __main__ -     Best mrr:0.784
10/09/2022 07:11:23 - INFO - __main__ -     ********************
10/09/2022 07:11:29 - INFO - __main__ -   Saving model checkpoint to saved_models/code_search/unixcoder/partial_freezing/ruby/freeze_bottom_0_layers/20221009070500/checkpoint-best-mrr/model.bin
10/09/2022 07:12:21 - INFO - __main__ -   epoch 3 step 100 loss 0.11254
10/09/2022 07:13:13 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:13:13 - INFO - __main__ -     Num queries = 1400
10/09/2022 07:13:13 - INFO - __main__ -     Num codes = 4360
10/09/2022 07:13:13 - INFO - __main__ -     Batch size = 128
10/09/2022 07:13:21 - INFO - __main__ -     R@1 = 0.698
10/09/2022 07:13:21 - INFO - __main__ -     R@5 = 0.884
10/09/2022 07:13:21 - INFO - __main__ -     R@10 = 0.922
10/09/2022 07:13:21 - INFO - __main__ -     eval_mrr = 0.78
10/09/2022 07:14:13 - INFO - __main__ -   epoch 4 step 100 loss 0.08457
10/09/2022 07:15:05 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:15:05 - INFO - __main__ -     Num queries = 1400
10/09/2022 07:15:05 - INFO - __main__ -     Num codes = 4360
10/09/2022 07:15:05 - INFO - __main__ -     Batch size = 128
10/09/2022 07:15:12 - INFO - __main__ -     R@1 = 0.697
10/09/2022 07:15:12 - INFO - __main__ -     R@5 = 0.884
10/09/2022 07:15:12 - INFO - __main__ -     R@10 = 0.917
10/09/2022 07:15:12 - INFO - __main__ -     eval_mrr = 0.78
10/09/2022 07:16:04 - INFO - __main__ -   epoch 5 step 100 loss 0.06496
10/09/2022 07:16:57 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:16:57 - INFO - __main__ -     Num queries = 1400
10/09/2022 07:16:57 - INFO - __main__ -     Num codes = 4360
10/09/2022 07:16:57 - INFO - __main__ -     Batch size = 128
10/09/2022 07:17:04 - INFO - __main__ -     R@1 = 0.697
10/09/2022 07:17:04 - INFO - __main__ -     R@5 = 0.883
10/09/2022 07:17:04 - INFO - __main__ -     R@10 = 0.916
10/09/2022 07:17:04 - INFO - __main__ -     eval_mrr = 0.779
10/09/2022 07:17:56 - INFO - __main__ -   epoch 6 step 100 loss 0.04938
10/09/2022 07:18:48 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:18:48 - INFO - __main__ -     Num queries = 1400
10/09/2022 07:18:48 - INFO - __main__ -     Num codes = 4360
10/09/2022 07:18:48 - INFO - __main__ -     Batch size = 128
10/09/2022 07:18:56 - INFO - __main__ -     R@1 = 0.694
10/09/2022 07:18:56 - INFO - __main__ -     R@5 = 0.882
10/09/2022 07:18:56 - INFO - __main__ -     R@10 = 0.918
10/09/2022 07:18:56 - INFO - __main__ -     eval_mrr = 0.777
10/09/2022 07:19:48 - INFO - __main__ -   epoch 7 step 100 loss 0.04338
10/09/2022 07:20:40 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:20:40 - INFO - __main__ -     Num queries = 1400
10/09/2022 07:20:40 - INFO - __main__ -     Num codes = 4360
10/09/2022 07:20:40 - INFO - __main__ -     Batch size = 128
10/09/2022 07:20:48 - INFO - __main__ -     R@1 = 0.699
10/09/2022 07:20:48 - INFO - __main__ -     R@5 = 0.884
10/09/2022 07:20:48 - INFO - __main__ -     R@10 = 0.918
10/09/2022 07:20:48 - INFO - __main__ -     eval_mrr = 0.779
10/09/2022 07:21:40 - INFO - __main__ -   epoch 8 step 100 loss 0.03881
10/09/2022 07:22:32 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:22:32 - INFO - __main__ -     Num queries = 1400
10/09/2022 07:22:32 - INFO - __main__ -     Num codes = 4360
10/09/2022 07:22:32 - INFO - __main__ -     Batch size = 128
10/09/2022 07:22:39 - INFO - __main__ -     R@1 = 0.701
10/09/2022 07:22:39 - INFO - __main__ -     R@5 = 0.883
10/09/2022 07:22:39 - INFO - __main__ -     R@10 = 0.916
10/09/2022 07:22:39 - INFO - __main__ -     eval_mrr = 0.78
10/09/2022 07:23:32 - INFO - __main__ -   epoch 9 step 100 loss 0.03562
10/09/2022 07:24:24 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:24:24 - INFO - __main__ -     Num queries = 1400
10/09/2022 07:24:24 - INFO - __main__ -     Num codes = 4360
10/09/2022 07:24:24 - INFO - __main__ -     Batch size = 128
10/09/2022 07:24:31 - INFO - __main__ -     R@1 = 0.697
10/09/2022 07:24:31 - INFO - __main__ -     R@5 = 0.884
10/09/2022 07:24:31 - INFO - __main__ -     R@10 = 0.918
10/09/2022 07:24:31 - INFO - __main__ -     eval_mrr = 0.779
10/09/2022 07:24:35 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:24:35 - INFO - __main__ -     Num queries = 1400
10/09/2022 07:24:35 - INFO - __main__ -     Num codes = 4360
10/09/2022 07:24:35 - INFO - __main__ -     Batch size = 128
10/09/2022 07:24:42 - INFO - __main__ -   ***** Eval results *****
10/09/2022 07:24:42 - INFO - __main__ -     R@1 = 0.702
10/09/2022 07:24:42 - INFO - __main__ -     R@10 = 0.919
10/09/2022 07:24:42 - INFO - __main__ -     R@5 = 0.889
10/09/2022 07:24:42 - INFO - __main__ -     eval_mrr = 0.784
10/09/2022 07:24:45 - INFO - __main__ -   ***** Running evaluation *****
10/09/2022 07:24:45 - INFO - __main__ -     Num queries = 1261
10/09/2022 07:24:45 - INFO - __main__ -     Num codes = 4360
10/09/2022 07:24:45 - INFO - __main__ -     Batch size = 128
10/09/2022 07:24:53 - INFO - __main__ -   ***** Eval results *****
10/09/2022 07:24:53 - INFO - __main__ -     R@1 = 0.665
10/09/2022 07:24:53 - INFO - __main__ -     R@10 = 0.904
10/09/2022 07:24:53 - INFO - __main__ -     R@5 = 0.862
10/09/2022 07:24:53 - INFO - __main__ -     eval_mrr = 0.751
10/09/2022 07:24:53 - INFO - utils -   saved dataset in saved_models/code_search/unixcoder/partial_freezing/ruby/freeze_bottom_0_layers/20221009070500/result.jsonl
