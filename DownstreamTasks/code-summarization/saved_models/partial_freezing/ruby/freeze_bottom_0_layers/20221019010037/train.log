10/19/2022 01:00:44 - INFO - __main__ -   device: cuda, n_gpu: 1
10/19/2022 01:00:45 - DEBUG - filelock -   Attempting to acquire lock 140493594818112 on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
10/19/2022 01:00:45 - DEBUG - filelock -   Lock 140493594818112 acquired on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
Downloading:   0%|          | 0.00/916k [00:00<?, ?B/s]Downloading:   9%|▉         | 84.0k/916k [00:00<00:00, 856kB/s]Downloading:  41%|████      | 372k/916k [00:00<00:00, 2.04MB/s]Downloading: 100%|██████████| 916k/916k [00:00<00:00, 3.63MB/s]
10/19/2022 01:00:45 - DEBUG - filelock -   Attempting to release lock 140493594818112 on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
10/19/2022 01:00:45 - DEBUG - filelock -   Lock 140493594818112 released on /home/aiscuser/.cache/huggingface/transformers/6537f24197db9749ad60f891d7a50ec2de3992bee193d25b24bb244ee5ca91f9.6243fbb3cc75148b68777473341e2d0860fde2b135f39c1d7d274d8ba1763e13.lock
10/19/2022 01:00:46 - DEBUG - filelock -   Attempting to acquire lock 140493593546560 on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
10/19/2022 01:00:46 - DEBUG - filelock -   Lock 140493593546560 acquired on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
Downloading:   0%|          | 0.00/434k [00:00<?, ?B/s]Downloading:   6%|▋         | 28.0k/434k [00:00<00:01, 259kB/s]Downloading:  43%|████▎     | 188k/434k [00:00<00:00, 968kB/s] Downloading: 100%|██████████| 434k/434k [00:00<00:00, 1.58MB/s]
10/19/2022 01:00:46 - DEBUG - filelock -   Attempting to release lock 140493593546560 on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
10/19/2022 01:00:46 - DEBUG - filelock -   Lock 140493593546560 released on /home/aiscuser/.cache/huggingface/transformers/e9a41c80e105c7ebfab8467fd5fa110db792fa435a42cf53fc84cd4dbce63203.fcaa28dbb04dd654a7ac023857de409e4815667a26706e2aa9a1bbc3ed49037a.lock
10/19/2022 01:00:47 - DEBUG - filelock -   Attempting to acquire lock 140493593546176 on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
10/19/2022 01:00:47 - DEBUG - filelock -   Lock 140493593546176 acquired on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]Downloading: 100%|██████████| 772/772 [00:00<00:00, 709kB/s]
10/19/2022 01:00:47 - DEBUG - filelock -   Attempting to release lock 140493593546176 on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
10/19/2022 01:00:47 - DEBUG - filelock -   Lock 140493593546176 released on /home/aiscuser/.cache/huggingface/transformers/192a4a8bfa30aa3013d375ea31db6b14b0f753bf61bd99b778cb8ebaa0d6a338.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0.lock
10/19/2022 01:00:47 - DEBUG - filelock -   Attempting to acquire lock 140493594818064 on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
10/19/2022 01:00:47 - DEBUG - filelock -   Lock 140493594818064 acquired on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
Downloading:   0%|          | 0.00/1.09k [00:00<?, ?B/s]Downloading: 100%|██████████| 1.09k/1.09k [00:00<00:00, 869kB/s]
10/19/2022 01:00:47 - DEBUG - filelock -   Attempting to release lock 140493594818064 on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
10/19/2022 01:00:47 - DEBUG - filelock -   Lock 140493594818064 released on /home/aiscuser/.cache/huggingface/transformers/74b423f29ba4f21ecd941f8d4fdc1e5a1568328f2d478850463813dc4e81c58a.ad8c4e4e357cd74df740cd60a08548a831bd19834e8802cfa73d289e1818a8c4.lock
10/19/2022 01:00:48 - DEBUG - filelock -   Attempting to acquire lock 140493492609856 on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
10/19/2022 01:00:48 - DEBUG - filelock -   Lock 140493492609856 acquired on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
Downloading:   0%|          | 0.00/691 [00:00<?, ?B/s]Downloading: 100%|██████████| 691/691 [00:00<00:00, 450kB/s]
10/19/2022 01:00:48 - DEBUG - filelock -   Attempting to release lock 140493492609856 on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
10/19/2022 01:00:48 - DEBUG - filelock -   Lock 140493492609856 released on /home/aiscuser/.cache/huggingface/transformers/f47f36c6d415b8e978f9685f6dbf2651cc9c951dea26b74fcf8bf62e44900449.b53aa458f35a3b932d45090e5916927053a2bf0e803f4eb410b7d1f922b60a05.lock
10/19/2022 01:00:49 - DEBUG - filelock -   Attempting to acquire lock 140493492609760 on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
10/19/2022 01:00:49 - DEBUG - filelock -   Lock 140493492609760 acquired on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
Downloading:   0%|          | 0.00/480M [00:00<?, ?B/s]Downloading:   1%|          | 2.85M/480M [00:00<00:16, 29.9MB/s]Downloading:   2%|▏         | 9.56M/480M [00:00<00:09, 53.4MB/s]Downloading:   4%|▎         | 16.9M/480M [00:00<00:07, 64.1MB/s]Downloading:   5%|▌         | 24.2M/480M [00:00<00:06, 69.1MB/s]Downloading:   7%|▋         | 32.5M/480M [00:00<00:06, 75.5MB/s]Downloading:   8%|▊         | 39.7M/480M [00:00<00:06, 74.8MB/s]Downloading:  10%|▉         | 46.8M/480M [00:00<00:06, 68.4MB/s]Downloading:  11%|█▏        | 54.6M/480M [00:00<00:06, 72.3MB/s]Downloading:  13%|█▎        | 62.0M/480M [00:00<00:05, 73.8MB/s]Downloading:  15%|█▍        | 70.5M/480M [00:01<00:05, 78.3MB/s]Downloading:  16%|█▌        | 78.0M/480M [00:01<00:06, 68.9MB/s]Downloading:  18%|█▊        | 84.8M/480M [00:01<00:06, 68.8MB/s]Downloading:  19%|█▉        | 92.0M/480M [00:01<00:05, 70.6MB/s]Downloading:  21%|██        | 101M/480M [00:01<00:05, 76.7MB/s] Downloading:  23%|██▎       | 108M/480M [00:01<00:05, 76.5MB/s]Downloading:  24%|██▍       | 116M/480M [00:01<00:04, 79.1MB/s]Downloading:  26%|██▌       | 124M/480M [00:01<00:05, 68.2MB/s]Downloading:  27%|██▋       | 132M/480M [00:01<00:05, 72.6MB/s]Downloading:  29%|██▉       | 139M/480M [00:02<00:04, 73.2MB/s]Downloading:  30%|███       | 146M/480M [00:02<00:05, 58.8MB/s]Downloading:  32%|███▏      | 154M/480M [00:02<00:05, 63.5MB/s]Downloading:  33%|███▎      | 161M/480M [00:02<00:05, 66.0MB/s]Downloading:  35%|███▌      | 169M/480M [00:02<00:04, 69.8MB/s]Downloading:  37%|███▋      | 176M/480M [00:02<00:04, 71.8MB/s]Downloading:  38%|███▊      | 184M/480M [00:02<00:04, 75.1MB/s]Downloading:  40%|███▉      | 191M/480M [00:02<00:04, 71.5MB/s]Downloading:  41%|████▏     | 199M/480M [00:02<00:03, 74.0MB/s]Downloading:  43%|████▎     | 206M/480M [00:03<00:04, 69.5MB/s]Downloading:  44%|████▍     | 213M/480M [00:03<00:03, 70.8MB/s]Downloading:  46%|████▌     | 220M/480M [00:03<00:04, 66.6MB/s]Downloading:  47%|████▋     | 227M/480M [00:03<00:04, 65.3MB/s]Downloading:  49%|████▉     | 234M/480M [00:03<00:03, 69.1MB/s]Downloading:  50%|█████     | 241M/480M [00:03<00:03, 63.0MB/s]Downloading:  51%|█████▏    | 247M/480M [00:03<00:03, 64.3MB/s]Downloading:  53%|█████▎    | 254M/480M [00:03<00:03, 63.8MB/s]Downloading:  54%|█████▍    | 261M/480M [00:03<00:03, 65.3MB/s]Downloading:  56%|█████▌    | 270M/480M [00:04<00:03, 72.2MB/s]Downloading:  58%|█████▊    | 277M/480M [00:04<00:03, 62.9MB/s]Downloading:  59%|█████▉    | 283M/480M [00:04<00:03, 64.4MB/s]Downloading:  60%|██████    | 290M/480M [00:04<00:02, 66.5MB/s]Downloading:  62%|██████▏   | 297M/480M [00:04<00:02, 66.8MB/s]Downloading:  63%|██████▎   | 304M/480M [00:04<00:02, 69.5MB/s]Downloading:  65%|██████▍   | 312M/480M [00:04<00:02, 73.5MB/s]Downloading:  67%|██████▋   | 321M/480M [00:04<00:02, 78.2MB/s]Downloading:  68%|██████▊   | 328M/480M [00:04<00:02, 74.8MB/s]Downloading:  70%|██████▉   | 336M/480M [00:05<00:01, 77.0MB/s]Downloading:  72%|███████▏  | 344M/480M [00:05<00:01, 80.4MB/s]Downloading:  73%|███████▎  | 352M/480M [00:05<00:01, 80.0MB/s]Downloading:  75%|███████▍  | 360M/480M [00:05<00:01, 81.6MB/s]Downloading:  77%|███████▋  | 368M/480M [00:05<00:01, 82.5MB/s]Downloading:  78%|███████▊  | 377M/480M [00:05<00:01, 84.4MB/s]Downloading:  80%|████████  | 385M/480M [00:05<00:01, 83.4MB/s]Downloading:  82%|████████▏ | 393M/480M [00:05<00:01, 80.4MB/s]Downloading:  84%|████████▎ | 401M/480M [00:05<00:01, 82.6MB/s]Downloading:  85%|████████▌ | 409M/480M [00:06<00:01, 66.5MB/s]Downloading:  87%|████████▋ | 418M/480M [00:06<00:00, 72.3MB/s]Downloading:  88%|████████▊ | 425M/480M [00:06<00:00, 64.8MB/s]Downloading:  90%|████████▉ | 432M/480M [00:06<00:00, 65.1MB/s]Downloading:  92%|█████████▏| 440M/480M [00:06<00:00, 70.4MB/s]Downloading:  93%|█████████▎| 447M/480M [00:06<00:00, 73.0MB/s]Downloading:  95%|█████████▍| 455M/480M [00:06<00:00, 73.8MB/s]Downloading:  96%|█████████▋| 463M/480M [00:06<00:00, 76.8MB/s]Downloading:  98%|█████████▊| 470M/480M [00:06<00:00, 77.1MB/s]Downloading: 100%|█████████▉| 479M/480M [00:07<00:00, 80.9MB/s]Downloading: 100%|██████████| 480M/480M [00:07<00:00, 71.7MB/s]
10/19/2022 01:00:56 - DEBUG - filelock -   Attempting to release lock 140493492609760 on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
10/19/2022 01:00:56 - DEBUG - filelock -   Lock 140493492609760 released on /home/aiscuser/.cache/huggingface/transformers/e472463826d959ba1a2526157c66c6678d307297de0ac70cb20d4bc20227a3ea.cd2d780fc8b692f148ec889e56ece5a353765aa429eda28d9a89b5a1aeb735db.lock
10/19/2022 01:00:58 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, beam_size=10, debug=False, dev_filename='dataset/ruby/valid.jsonl', device=device(type='cuda'), do_eval=True, do_test=True, do_train=True, eval_batch_size=256, freeze_bottom_k_layer_index=0, gradient_accumulation_steps=1, learning_rate=5e-05, max_grad_norm=1.0, max_source_length=256, max_target_length=128, model_name_or_path='microsoft/unixcoder-base', n_debug_samples=100, n_gpu=1, no_cuda=False, num_train_epochs=10, output_dir='saved_models/code_sum/unixcoder/partial_freezing/ruby/freeze_bottom_0_layers/20221019010037', seed=123456, test_filename='dataset/ruby/test.jsonl', train_batch_size=32, train_filename='dataset/ruby/train.jsonl', weight_decay=0.0)
10/19/2022 01:00:58 - INFO - __main__ -   +------------------------------------------------------------+--------------+---------+
| Layer Name                                                 | Output Shape | Param # |
+------------------------------------------------------------+--------------+---------+
| encoder.encoder.layer.0.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.0.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.0.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.0.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.0.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.0.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.0.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.0.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.0.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.0.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.0.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.0.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.0.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.0.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.0.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.0.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.1.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.1.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.1.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.1.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.1.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.1.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.1.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.1.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.1.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.1.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.1.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.1.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.1.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.1.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.1.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.1.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.2.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.2.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.2.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.2.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.2.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.2.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.2.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.2.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.2.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.2.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.2.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.2.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.2.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.2.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.2.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.2.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.3.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.3.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.3.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.3.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.3.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.3.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.3.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.3.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.3.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.3.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.3.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.3.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.3.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.3.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.3.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.3.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.4.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.4.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.4.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.4.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.4.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.4.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.4.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.4.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.4.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.4.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.4.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.4.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.4.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.4.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.4.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.4.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.5.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.5.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.5.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.5.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.5.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.5.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.5.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.5.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.5.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.5.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.5.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.5.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.5.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.5.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.5.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.5.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.6.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.6.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.6.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.6.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.6.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.6.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.6.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.6.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.6.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.6.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.6.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.6.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.6.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.7.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.7.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.7.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.7.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.7.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.7.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.7.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.7.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.7.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.7.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.7.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.7.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.7.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.8.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.8.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.8.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.8.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.8.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.8.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.8.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.8.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.8.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.8.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.8.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.8.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.8.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.9.attention.self.query.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.self.query.bias          |        [768] |     768 |
| encoder.encoder.layer.9.attention.self.key.weight          |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.self.key.bias            |        [768] |     768 |
| encoder.encoder.layer.9.attention.self.value.weight        |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.self.value.bias          |        [768] |     768 |
| encoder.encoder.layer.9.attention.output.dense.weight      |   [768, 768] |  589824 |
| encoder.encoder.layer.9.attention.output.dense.bias        |        [768] |     768 |
| encoder.encoder.layer.9.attention.output.LayerNorm.weight  |        [768] |     768 |
| encoder.encoder.layer.9.attention.output.LayerNorm.bias    |        [768] |     768 |
| encoder.encoder.layer.9.intermediate.dense.weight          |  [3072, 768] | 2359296 |
| encoder.encoder.layer.9.intermediate.dense.bias            |       [3072] |    3072 |
| encoder.encoder.layer.9.output.dense.weight                |  [768, 3072] | 2359296 |
| encoder.encoder.layer.9.output.dense.bias                  |        [768] |     768 |
| encoder.encoder.layer.9.output.LayerNorm.weight            |        [768] |     768 |
| encoder.encoder.layer.9.output.LayerNorm.bias              |        [768] |     768 |
| encoder.encoder.layer.10.attention.self.query.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.self.query.bias         |        [768] |     768 |
| encoder.encoder.layer.10.attention.self.key.weight         |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.self.key.bias           |        [768] |     768 |
| encoder.encoder.layer.10.attention.self.value.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.self.value.bias         |        [768] |     768 |
| encoder.encoder.layer.10.attention.output.dense.weight     |   [768, 768] |  589824 |
| encoder.encoder.layer.10.attention.output.dense.bias       |        [768] |     768 |
| encoder.encoder.layer.10.attention.output.LayerNorm.weight |        [768] |     768 |
| encoder.encoder.layer.10.attention.output.LayerNorm.bias   |        [768] |     768 |
| encoder.encoder.layer.10.intermediate.dense.weight         |  [3072, 768] | 2359296 |
| encoder.encoder.layer.10.intermediate.dense.bias           |       [3072] |    3072 |
| encoder.encoder.layer.10.output.dense.weight               |  [768, 3072] | 2359296 |
| encoder.encoder.layer.10.output.dense.bias                 |        [768] |     768 |
| encoder.encoder.layer.10.output.LayerNorm.weight           |        [768] |     768 |
| encoder.encoder.layer.10.output.LayerNorm.bias             |        [768] |     768 |
| encoder.encoder.layer.11.attention.self.query.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.self.query.bias         |        [768] |     768 |
| encoder.encoder.layer.11.attention.self.key.weight         |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.self.key.bias           |        [768] |     768 |
| encoder.encoder.layer.11.attention.self.value.weight       |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.self.value.bias         |        [768] |     768 |
| encoder.encoder.layer.11.attention.output.dense.weight     |   [768, 768] |  589824 |
| encoder.encoder.layer.11.attention.output.dense.bias       |        [768] |     768 |
| encoder.encoder.layer.11.attention.output.LayerNorm.weight |        [768] |     768 |
| encoder.encoder.layer.11.attention.output.LayerNorm.bias   |        [768] |     768 |
| encoder.encoder.layer.11.intermediate.dense.weight         |  [3072, 768] | 2359296 |
| encoder.encoder.layer.11.intermediate.dense.bias           |       [3072] |    3072 |
| encoder.encoder.layer.11.output.dense.weight               |  [768, 3072] | 2359296 |
| encoder.encoder.layer.11.output.dense.bias                 |        [768] |     768 |
| encoder.encoder.layer.11.output.LayerNorm.weight           |        [768] |     768 |
| encoder.encoder.layer.11.output.LayerNorm.bias             |        [768] |     768 |
| encoder.pooler.dense.weight                                |   [768, 768] |  589824 |
| encoder.pooler.dense.bias                                  |        [768] |     768 |
| dense.weight                                               |   [768, 768] |  589824 |
| dense.bias                                                 |        [768] |     768 |
+------------------------------------------------------------+--------------+---------+
10/19/2022 01:00:58 - INFO - __main__ -   The model has 86235648 trainable parameters
10/19/2022 01:01:00 - INFO - __main__ -   *** Example ***
10/19/2022 01:01:00 - INFO - __main__ -   idx: 0
10/19/2022 01:01:00 - INFO - __main__ -   source_tokens: ['<s>', '<encoder-decoder>', '</s>', '<mask0>', 'def', '_render', '_', 'body', '_(', '_context', '_,', '_options', '_)', '_if', '_options', '_.', '_key', '?', '_(', '_:', 'partial', '_)', '_[', '_render', '_', 'partial', '_(', '_context', '_,', '_options', '_)', '_]', '_else', '_Streaming', 'Template', 'Renderer', '_.', '_new', '_(', '_@', 'lookup', '_', 'context', '_)', '_.', '_render', '_(', '_context', '_,', '_options', '_)', '_end', '_end', '</s>']
10/19/2022 01:01:00 - INFO - __main__ -   source_ids: 0 5 2 19 729 4342 181 1995 400 1552 2019 1466 743 462 1466 746 1129 149 400 545 7609 743 626 4342 181 7609 400 1552 2019 1466 743 2406 669 47128 3057 6412 746 579 400 890 4961 181 1499 743 746 4342 400 1552 2019 1466 743 1013 1013 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/19/2022 01:01:00 - INFO - __main__ -   target_tokens: ['<mask0>', 'Render', '_but', '_returns', '_a', '_valid', '_R', 'ack', '_body', '_.', '_If', '_fib', 'ers', '_are', '_defined', '_we', '_return', '_a', '_streaming', '_body', '_that', '_renders', '_the', '_template', '_piece', '_by', '_piece', '_.', '</s>']
10/19/2022 01:01:00 - INFO - __main__ -   target_ids: 19 3726 2107 2060 434 1976 821 598 3444 746 1359 24766 560 1147 3474 937 483 434 22676 3444 922 40840 448 3636 18781 1243 18781 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/19/2022 01:01:00 - INFO - __main__ -   *** Example ***
10/19/2022 01:01:00 - INFO - __main__ -   idx: 1
10/19/2022 01:01:00 - INFO - __main__ -   source_tokens: ['<s>', '<encoder-decoder>', '</s>', '<mask0>', 'def', '_attribute', '_', 'missing', '_(', '_match', '_,', '_*', '_args', '_,', '_&', '_block', '_)', '___', 'send', '__', '_(', '_match', '_.', '_target', '_,', '_match', '_.', '_attr', '_', 'name', '_,', '_args', '_,', '_block', '_)', '_end', '</s>']
10/19/2022 01:01:00 - INFO - __main__ -   source_ids: 0 5 2 19 729 2416 181 8487 400 1655 2019 426 1822 2019 519 1818 743 1267 2414 876 400 1655 746 1744 2019 1655 746 3526 181 616 2019 1822 2019 1818 743 1013 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/19/2022 01:01:00 - INFO - __main__ -   target_tokens: ['<mask0>', '+', '_attribute', '_', 'missing', '_+', '_is', '_like', '_+', '_method', '_', 'missing', '_+', '_but', '_for', '_attributes', '_.', '_When', '_+', '_method', '_', 'missing', '_+', '_is', '_called', '_we', '_check', '_to', '_see', '_if', '_there', '_is', '_a', '_matching', '_attribute', '_method', '_.', '_If', '_so', '_we', '_tell', '_+', '_attribute', '_', 'missing', '_+', '_to', '_dispatch', '_the', '_attribute', '_.', '_This', '_method', '_can', '_be', '_overloaded', '_to', '_customize', '_the', '_behavior', '_.', '</s>']
10/19/2022 01:01:00 - INFO - __main__ -   target_ids: 19 129 2416 181 8487 513 555 4401 513 1454 181 8487 513 2107 563 4402 746 5919 513 1454 181 8487 513 555 2953 937 1382 508 3986 462 2550 555 434 6506 2416 1454 746 1359 1769 937 11931 513 2416 181 8487 513 508 9363 448 2416 746 1600 1454 1347 661 45869 508 36145 448 9050 746 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
10/19/2022 01:01:15 - INFO - __main__ -   ***** Running training *****
10/19/2022 01:01:15 - INFO - __main__ -     Num examples = 24927
10/19/2022 01:01:15 - INFO - __main__ -     Batch size = 32
10/19/2022 01:01:15 - INFO - __main__ -     Num epoch = 10
10/19/2022 01:01:32 - INFO - __main__ -   epoch 0 step 100 loss 3.0342
10/19/2022 01:01:49 - INFO - __main__ -   epoch 0 step 200 loss 2.6626
10/19/2022 01:02:05 - INFO - __main__ -   epoch 0 step 300 loss 2.652
10/19/2022 01:02:22 - INFO - __main__ -   epoch 0 step 400 loss 2.6213
10/19/2022 01:02:38 - INFO - __main__ -   epoch 0 step 500 loss 2.5766
10/19/2022 01:02:55 - INFO - __main__ -   epoch 0 step 600 loss 2.5943
10/19/2022 01:03:11 - INFO - __main__ -   epoch 0 step 700 loss 2.6138
10/19/2022 01:03:26 - INFO - __main__ -   
***** Running evaluation *****
10/19/2022 01:03:26 - INFO - __main__ -     Num examples = 1400
10/19/2022 01:03:26 - INFO - __main__ -     Batch size = 256
10/19/2022 01:03:28 - INFO - __main__ -     eval_ppl = 16.08111
10/19/2022 01:03:28 - INFO - __main__ -     ********************
/sci_1/t-enshengshi/interpretability/sync_repo/bertviz/code-summarization/model.py:189: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  prevK = bestScoresId // numWords
Total: 1000
10/19/2022 01:05:49 - INFO - __main__ -     bleu-4 = 16.01 
10/19/2022 01:05:49 - INFO - __main__ -     ********************
10/19/2022 01:05:49 - INFO - __main__ -     Best bleu:16.01
10/19/2022 01:05:49 - INFO - __main__ -     ********************
10/19/2022 01:06:05 - INFO - __main__ -   epoch 1 step 800 loss 2.5559
10/19/2022 01:06:22 - INFO - __main__ -   epoch 1 step 900 loss 2.3777
10/19/2022 01:06:38 - INFO - __main__ -   epoch 1 step 1000 loss 2.4134
10/19/2022 01:06:55 - INFO - __main__ -   epoch 1 step 1100 loss 2.3748
10/19/2022 01:07:11 - INFO - __main__ -   epoch 1 step 1200 loss 2.3904
10/19/2022 01:07:28 - INFO - __main__ -   epoch 1 step 1300 loss 2.4398
10/19/2022 01:07:44 - INFO - __main__ -   epoch 1 step 1400 loss 2.4101
10/19/2022 01:08:01 - INFO - __main__ -   epoch 1 step 1500 loss 2.4074
10/19/2022 01:08:10 - INFO - __main__ -   
***** Running evaluation *****
10/19/2022 01:08:10 - INFO - __main__ -     Num examples = 1400
10/19/2022 01:08:10 - INFO - __main__ -     Batch size = 256
10/19/2022 01:08:13 - INFO - __main__ -     eval_ppl = 16.68998
10/19/2022 01:08:13 - INFO - __main__ -     ********************
Total: 1000
10/19/2022 01:10:38 - INFO - __main__ -     bleu-4 = 16.12 
10/19/2022 01:10:38 - INFO - __main__ -     ********************
10/19/2022 01:10:38 - INFO - __main__ -     Best bleu:16.12
10/19/2022 01:10:38 - INFO - __main__ -     ********************
10/19/2022 01:10:57 - INFO - __main__ -   epoch 2 step 1600 loss 2.2398
10/19/2022 01:11:13 - INFO - __main__ -   epoch 2 step 1700 loss 2.056
10/19/2022 01:11:30 - INFO - __main__ -   epoch 2 step 1800 loss 2.0791
10/19/2022 01:11:46 - INFO - __main__ -   epoch 2 step 1900 loss 2.0779
10/19/2022 01:12:02 - INFO - __main__ -   epoch 2 step 2000 loss 2.0887
10/19/2022 01:12:19 - INFO - __main__ -   epoch 2 step 2100 loss 2.0839
10/19/2022 01:12:35 - INFO - __main__ -   epoch 2 step 2200 loss 2.1048
10/19/2022 01:12:52 - INFO - __main__ -   epoch 2 step 2300 loss 2.0878
10/19/2022 01:12:58 - INFO - __main__ -   
***** Running evaluation *****
10/19/2022 01:12:58 - INFO - __main__ -     Num examples = 1400
10/19/2022 01:12:58 - INFO - __main__ -     Batch size = 256
10/19/2022 01:13:00 - INFO - __main__ -     eval_ppl = 17.95627
10/19/2022 01:13:00 - INFO - __main__ -     ********************
Total: 1000
10/19/2022 01:15:23 - INFO - __main__ -     bleu-4 = 15.09 
10/19/2022 01:15:23 - INFO - __main__ -     ********************
10/19/2022 01:15:33 - INFO - __main__ -   epoch 3 step 2400 loss 1.8992
10/19/2022 01:15:50 - INFO - __main__ -   epoch 3 step 2500 loss 1.7924
10/19/2022 01:16:06 - INFO - __main__ -   epoch 3 step 2600 loss 1.7994
10/19/2022 01:16:23 - INFO - __main__ -   epoch 3 step 2700 loss 1.8246
10/19/2022 01:16:39 - INFO - __main__ -   epoch 3 step 2800 loss 1.8232
10/19/2022 01:16:56 - INFO - __main__ -   epoch 3 step 2900 loss 1.7884
10/19/2022 01:17:12 - INFO - __main__ -   epoch 3 step 3000 loss 1.818
10/19/2022 01:17:29 - INFO - __main__ -   epoch 3 step 3100 loss 1.7903
10/19/2022 01:17:31 - INFO - __main__ -   
***** Running evaluation *****
10/19/2022 01:17:31 - INFO - __main__ -     Num examples = 1400
10/19/2022 01:17:31 - INFO - __main__ -     Batch size = 256
10/19/2022 01:17:34 - INFO - __main__ -     eval_ppl = 20.95517
10/19/2022 01:17:34 - INFO - __main__ -     ********************
Total: 1000
10/19/2022 01:20:11 - INFO - __main__ -     bleu-4 = 14.96 
10/19/2022 01:20:11 - INFO - __main__ -     ********************
  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [00:38<02:32, 38.03s/it] 40%|████      | 2/5 [01:13<01:49, 36.43s/it] 60%|██████    | 3/5 [01:51<01:14, 37.04s/it] 80%|████████  | 4/5 [02:26<00:36, 36.22s/it]100%|██████████| 5/5 [02:58<00:00, 34.80s/it]100%|██████████| 5/5 [02:58<00:00, 35.67s/it]
Total: 1261
10/19/2022 01:23:12 - INFO - __main__ -     bleu-4 = 14.91 
10/19/2022 01:23:12 - INFO - __main__ -     ********************
10/19/2022 01:23:13 - INFO - utils -   saved dataset in saved_models/code_sum/unixcoder/partial_freezing/ruby/freeze_bottom_0_layers/20221019010037/result.jsonl
